{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "095c5c57",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import OrderedDict\n",
    "from typing import List, Tuple, Optional\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms as transforms\n",
    "from torch.optim import SGD\n",
    "from datasets.utils.logging import disable_progress_bar\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "import flwr\n",
    "from flwr.client import Client, ClientApp, NumPyClient\n",
    "from flwr.common import Metrics, Context\n",
    "from flwr.server import ServerApp, ServerConfig, ServerAppComponents\n",
    "from flwr.server.strategy import FedAvg, Strategy\n",
    "from flwr.simulation import run_simulation\n",
    "from flwr_datasets import FederatedDataset\n",
    "\n",
    "from fedlearn.model import SmallCNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "c3e0686c",
   "metadata": {},
   "outputs": [],
   "source": [
    "datadir = Path().cwd().parent / \"data\" / \"flower_dataset\"\n",
    "logdir = Path().cwd().parent / \"logs\" / \"scaffold\"\n",
    "\n",
    "if not logdir.exists():\n",
    "    logdir.mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "002a2a64",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training on cuda\n",
      "Flower 1.18.0 / PyTorch 2.6.0+cu126\n"
     ]
    }
   ],
   "source": [
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Training on {DEVICE}\")\n",
    "print(f\"Flower {flwr.__version__} / PyTorch {torch.__version__}\")\n",
    "disable_progress_bar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e393fec1",
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_CLIENTS = 10\n",
    "BATCH_SIZE = 32\n",
    "\n",
    "\n",
    "def load_datasets(partition_id: int):\n",
    "    fds = FederatedDataset(\n",
    "        dataset=\"cifar10\", \n",
    "        partitioners={\"train\": NUM_CLIENTS},\n",
    "        cache_dir=datadir,\n",
    "        )\n",
    "    \n",
    "    partition = fds.load_partition(partition_id)\n",
    "    # Divide data on each node: 80% train, 20% test\n",
    "    partition_train_test = partition.train_test_split(test_size=0.2, seed=42)\n",
    "    pytorch_transforms = transforms.Compose(\n",
    "        [transforms.ToTensor(), transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))]\n",
    "    )\n",
    "\n",
    "    def apply_transforms(batch):\n",
    "        # Instead of passing transforms to CIFAR10(..., transform=transform)\n",
    "        # we will use this function to dataset.with_transform(apply_transforms)\n",
    "        # The transforms object is exactly the same\n",
    "        batch[\"img\"] = [pytorch_transforms(img) for img in batch[\"img\"]]\n",
    "        return batch\n",
    "\n",
    "    # Create train/val for each partition and wrap it into DataLoader\n",
    "    partition_train_test = partition_train_test.with_transform(apply_transforms)\n",
    "    trainloader = DataLoader(\n",
    "        partition_train_test[\"train\"], batch_size=BATCH_SIZE, shuffle=True\n",
    "    )\n",
    "    valloader = DataLoader(partition_train_test[\"test\"], batch_size=BATCH_SIZE)\n",
    "    testset = fds.load_split(\"test\").with_transform(apply_transforms)\n",
    "    testloader = DataLoader(testset, batch_size=BATCH_SIZE)\n",
    "    return trainloader, valloader, testloader"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fbbce0a",
   "metadata": {},
   "source": [
    "### Define Scaffold Optimizer\n",
    "\n",
    "Recall that the local update in Scaffold is given by\n",
    "\n",
    "$$\n",
    "w^{(i)} \\gets w^{(i)} - \\eta_l \\left( g_i(w^{(i)}) + c - c_i \\right)\n",
    "$$\n",
    "\n",
    "Which can be seen as a gradient correction to Stochastic Gradient Descent (SGD). We may therefore extend the pytorch ```SGD``` class. We do this by computing the the regular SGD step, then adding the correction manually:\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "w^{(i)} &\\gets w^{(i)} - \\eta_l \\, g_i\\left(w^{(i)}\\right) \\\\\n",
    "w^{(i)} &\\gets w^{(i)} - \\eta_l (c - c_i)\n",
    "\\end{align*}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b999b922",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ScaffoldOptimizer(SGD):\n",
    "    def __init__(self, params, lr=0.01, momentum=0.9, weight_decay=0):\n",
    "        super().__init__(params, lr, momentum, weight_decay)\n",
    "\n",
    "    def step(self, global_cv, client_cv):\n",
    "        \"\"\"\n",
    "        Perform a single optimization step.\n",
    "        :param global_cv: Global control variable\n",
    "        :param client_cv: Client control variable\n",
    "        \"\"\"\n",
    "        # compute regular SGD step\n",
    "        #   w <- w - lr * grad\n",
    "        super().step() \n",
    "\n",
    "        # now add the correction term\n",
    "        #   w <- w - lr * (g_cv - c_cv)\n",
    "        for group in self.param_groups:\n",
    "            for param, g_cv, c_cv in zip(group[\"params\"], global_cv, client_cv):\n",
    "                # here we add the correction term to each parameter tensor.\n",
    "                # the alpha value scales the correction term\n",
    "                param.data.add_(g_cv - c_cv, alpha=-group[\"lr\"]) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0954175b",
   "metadata": {},
   "source": [
    "We can now write a function for the local training. In this function, we want simply want to perform gradient corrected SGD updates over the local data for $E$ epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "862d0b68",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_scaffold(net: torch.nn.Module, \n",
    "                   device: torch.device, \n",
    "                   trainloader: torch.utils.data.DataLoader,\n",
    "                   criterion: nn.Module,\n",
    "                   num_epochs: int, \n",
    "                   lr: float, \n",
    "                   momentum: float, \n",
    "                   weight_decay: float, \n",
    "                   global_cv: List[torch.Tensor], \n",
    "                   client_cv: List[torch.Tensor],\n",
    "                   ) -> None:\n",
    "    \"\"\"\n",
    "    Function that trains a model using the Scaffold optimization algorithm.\n",
    "    Parameters:\n",
    "        net:            The neural network model to train.\n",
    "        device:         The device to run the training on (CPU or GPU).\n",
    "        trainloader:    DataLoader for the training data.\n",
    "        criterion:      Loss function to use for training.\n",
    "        num_epochs:     Number of epochs to train the model.\n",
    "        lr:             Learning rate for the optimizer.\n",
    "        momentum:       Momentum factor for the optimizer.\n",
    "        weight_decay:   Weight decay (L2 penalty) for the optimizer.\n",
    "        global_cv:      Global control variables for Scaffold.\n",
    "        client_cv:      Client control variables for Scaffold.\n",
    "    \"\"\"\n",
    "    \n",
    "    net.train()\n",
    "    optimizer = ScaffoldOptimizer(\n",
    "        net.parameters(), lr=lr, momentum=momentum, weight_decay=weight_decay\n",
    "    )\n",
    "    \n",
    "    for _ in range(num_epochs):\n",
    "        for Xtrain, Ytrain in trainloader:\n",
    "            Xtrain, Ytrain = Xtrain.to(device), Ytrain.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            output = net(Xtrain)\n",
    "            loss = criterion(output, Ytrain)\n",
    "            loss.backward()\n",
    "            \n",
    "            # Perform a single optimization step with the control variables\n",
    "            optimizer.step(global_cv, client_cv)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "436bd857",
   "metadata": {},
   "source": [
    "We will also define a test function, which will be called to evaluate our model. This will give us some metrics to evaluate the performance of the model. As we are working with a classifier, we are interested in both the loss and accuracy of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8975daea",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(net: torch.nn.Module, \n",
    "         device: torch.device, \n",
    "         testloader: torch.utils.data.DataLoader,\n",
    "         criterion: nn.Module,\n",
    "         ) -> Tuple[float, float]:\n",
    "    \"\"\"\n",
    "    Function that tests a model on the test dataset.\n",
    "    Parameters:\n",
    "        net:        The neural network model to test.\n",
    "        device:     The device to run the testing on (CPU or GPU).\n",
    "        testloader: DataLoader for the test data.\n",
    "        criterion:  Loss function to use for testing.\n",
    "    Returns:\n",
    "        Tuple containing the average loss and accuracy on the test set.\n",
    "    \"\"\"\n",
    "    \n",
    "    net.eval()\n",
    "    total_loss = 0.0    # Accumulator for total loss\n",
    "    correct = 0         # tracker for correct predictions\n",
    "    total = 0           # tracker for total predictions\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for Xtest, Ytest in testloader:\n",
    "            Xtest, Ytest = Xtest.to(device), Ytest.to(device)\n",
    "            output = net(Xtest)\n",
    "            loss = criterion(output, Ytest)\n",
    "            total_loss += loss.item()\n",
    "            _, predicted = output.max(1)\n",
    "            total += Ytest.size(0)\n",
    "            correct += predicted.eq(Ytest).sum().item()\n",
    "    \n",
    "    avg_loss = total_loss / len(testloader) # compute the average loss\n",
    "    accuracy = correct / total              # compute the accuracy\n",
    "    return avg_loss, accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26148887",
   "metadata": {},
   "source": [
    "For the simulation, we will use the flower framework, which was introduced in the _ notebook. To do so, we need to specify both Client and Server classes. We need to consider a couple of things: \n",
    "1. We can inherit the  ```NumPyClient``` class from the flower framework, however we need to remember to convert between ```np.ndarray``` and ```torch.tensor``` before and after local updates.\n",
    "2. We need to specify a ```client.fit()``` method, containing all the logic for the local update. This method has 2 inputs\n",
    "   1. parameters: a list of ```np.ndarray```, containing both global model parameters and global control variates\n",
    "   2. config: dict for specifying training configuration (we will ignore this for now)\n",
    "3. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8f6d009c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ScaffoldClient(NumPyClient):\n",
    "    def __init__(self, \n",
    "                 cid: int, \n",
    "                 net: torch.nn.Module, \n",
    "                 trainloader: torch.utils.data.DataLoader, \n",
    "                 valloader: torch.utils.data.DataLoader,\n",
    "                 criterion: nn.Module,\n",
    "                 device: torch.device,\n",
    "                 num_epochs: int,\n",
    "                 lr: float,\n",
    "                 momentum: float,\n",
    "                 weight_decay: float,\n",
    "                 save_dir: Optional[str] = None,\n",
    "                 ):\n",
    "        self.cid = cid\n",
    "        self.net = net\n",
    "        self.trainloader = trainloader\n",
    "        self.valloader = valloader\n",
    "        self.criterion = criterion\n",
    "        self.device = device\n",
    "        self.num_epochs = num_epochs\n",
    "        self.lr = lr\n",
    "        self.momentum = momentum\n",
    "        self.weight_decay = weight_decay\n",
    "\n",
    "        # define directory to save client control variates\n",
    "        if save_dir is None:\n",
    "            save_dir = \"client_cvs\"\n",
    "\n",
    "        # create directory if it does not exist\n",
    "        if not os.path.exists(save_dir):\n",
    "            os.makedirs(save_dir)\n",
    "\n",
    "        # define the path to save the client control variates\n",
    "        self.save_name = os.path.join(save_dir, f\"client_{self.cid}_cv.pt\")\n",
    "\n",
    "        # initialize client control variates\n",
    "        self.client_cv = [torch.zeros(param.shape) for param in self.net.parameters()]\n",
    "\n",
    "    # define methods required by NumPyClient interface\n",
    "    def get_parameters(self) -> List[np.ndarray]:\n",
    "        return [val.cpu().numpy() for _, val in self.model.state_dict().items()]\n",
    "\n",
    "    def set_parameters(self, parameters: List[np.ndarray]) -> None:\n",
    "        params_dict = zip(self.net.state_dict().keys(), parameters)\n",
    "        state_dict = OrderedDict({k : torch.tensor(v) for k, v in params_dict})\n",
    "        self.net.load_state_dict(state_dict, strict=True)\n",
    "\n",
    "    # Here is where all the training logic and control variate updates happen\n",
    "    def fit(self, parameters: List[np.ndarray], config: dict) -> Tuple[List[np.ndarray], int, dict]:\n",
    "        self.set_parameters(parameters)\n",
    "\n",
    "        # the global parameters are packed together with the global control variates\n",
    "        # in the form [params, global_cv]. we start by separating them\n",
    "        params = parameters[:len(parameters) // 2]\n",
    "        global_cv = parameters[len(parameters) // 2:]\n",
    "\n",
    "        # load the current global model:\n",
    "        self.set_parameters(params)\n",
    "\n",
    "        # load client control variates, if they exist:\n",
    "        if os.path.exists(self.save_name):\n",
    "            self.client_cv = torch.load(self.save_name)\n",
    "\n",
    "        # convert global control variates to tensors\n",
    "        global_cv = [torch.tensor(cv) for cv in global_cv]\n",
    "\n",
    "        # call the training function\n",
    "        train_scaffold(\n",
    "            net=self.net,\n",
    "            device=self.device,\n",
    "            trainloader=self.trainloader,\n",
    "            criterion=self.criterion,\n",
    "            num_epochs=self.num_epochs,\n",
    "            lr=self.lr,\n",
    "            momentum=self.momentum,\n",
    "            weight_decay=self.weight_decay,\n",
    "            global_cv=global_cv,\n",
    "            client_cv=self.client_cv\n",
    "        )\n",
    "\n",
    "        # update the client control variates\n",
    "        yi = self.get_parameters()\n",
    "        client_cv_new = []\n",
    "\n",
    "        # compute coefficient for the control variates\n",
    "        # 1 / (K * eta) where K is the number of backward passes (num_epochs * len(trainloader))\n",
    "        coeff = 1. / (self.num_epochs * len(self.trainloader) * self.lr) \n",
    "\n",
    "        # compute client control variate update\n",
    "        for xj, yj, cj, cij in zip(params, yi, global_cv, self.client_cv):\n",
    "            client_cv_new.append(\n",
    "                cij - cj - coeff * (xj - yj)\n",
    "            ) \n",
    "\n",
    "        # compute server updates\n",
    "        server_update_x = [xj - yj for xj, yj in zip(params, yi)]\n",
    "        server_update_c = [cij_n - cij for cij_n, cij in zip(client_cv_new, self.client_cv)]\n",
    "\n",
    "        self.client_cv = client_cv_new\n",
    "        torch.save(self.client_cv, self.save_name)\n",
    "\n",
    "        #concatenate server updates\n",
    "        server_update = server_update_x + server_update_c\n",
    "\n",
    "        return server_update, len(self.trainloader.dataset), {}\n",
    "\n",
    "\n",
    "\n",
    "    def evaluate(self, parameters: List[np.ndarray], config: dict) -> Tuple[float, int, dict]:\n",
    "        self.set_parameters(parameters)\n",
    "        avg_loss, accuracy = test(\n",
    "            net=self.net,\n",
    "            device=self.device,\n",
    "            testloader=self.valloader,\n",
    "            criterion=self.criterion\n",
    "        )\n",
    "        return float(avg_loss), len(self.testloader), {\"accuracy\": accuracy}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7644bf3c",
   "metadata": {},
   "source": [
    "Now that we have the flower client defined, we need to define a constructor function which the flower framework can use to instatiate clients as it goes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fc2badb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def client_fn(context: Context) -> Client:\n",
    "    cid = context.node_config[\"partition\"]\n",
    "    trainloader, valloader, testloader = load_datasets(cid)\n",
    "\n",
    "    net = SmallCNN().to(DEVICE)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    \n",
    "    # Define hyperparameters for training\n",
    "    num_epochs = 1\n",
    "    lr = 0.01\n",
    "    momentum = 0.9\n",
    "    weight_decay = 0.0005\n",
    "\n",
    "    return ScaffoldClient(\n",
    "        cid=cid,\n",
    "        net=net,\n",
    "        trainloader=trainloader,\n",
    "        valloader=valloader,\n",
    "        criterion=criterion,\n",
    "        device=DEVICE,\n",
    "        num_epochs=num_epochs,\n",
    "        lr=lr,\n",
    "        momentum=momentum,\n",
    "        weight_decay=weight_decay,\n",
    "        save_dir=\"client_cvs\"\n",
    "    )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "FLenv2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
