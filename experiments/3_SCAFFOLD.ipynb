{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4f9c779e",
   "metadata": {},
   "source": [
    "# SCAFFOLD for Non-IID Setting \n",
    "\n",
    "## Introduction\n",
    "\n",
    "In this notebook we implement the SCAFFOLD algorithm, as proposed by Karimireddy et al. in the paper [SCAFFOLD: Stochastic Controlled Averaging for Federated Learning](https://proceedings.mlr.press/v119/karimireddy20a/karimireddy20a.pdf) in 2020. The general idea is to introduce control variates to the local model updates, in order to mitigate the effects that data heterogeneity has on the learning process. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "095c5c57",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Tuple, Optional, Dict\n",
    "from pathlib import Path\n",
    "from logging import WARN, INFO, DEBUG\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.optim import SGD\n",
    "from datasets.utils.logging import disable_progress_bar\n",
    "\n",
    "import flwr\n",
    "from flwr.client import Client, ClientApp, NumPyClient\n",
    "from flwr.common import (\n",
    "    Metrics,\n",
    "    Parameters,\n",
    "    Context, \n",
    "    Code,\n",
    "    ArrayRecord,\n",
    "    Scalar, \n",
    "    ndarrays_to_parameters,\n",
    "    parameters_to_ndarrays,\n",
    "    )\n",
    "from flwr.common.typing import GetParametersIns\n",
    "from flwr.common.logger import log\n",
    "from flwr.server import ServerApp, ServerConfig, ServerAppComponents, Server\n",
    "from flwr.server.strategy import Strategy, FedAvg\n",
    "from flwr.server.client_manager import ClientManager, SimpleClientManager\n",
    "from flwr.server.server import FitResultsAndFailures, fit_clients\n",
    "from flwr.simulation import run_simulation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcf16d73",
   "metadata": {},
   "source": [
    "Next, we need to make some imports from our own package. We will import the ```SmallCNN``` neural net object, as this is what we are running all the tests with. We will also import the function ```load_datasets```, which returns a tuple of ```(trainloader,valloader,testloader)```. We will again make use of the ```test``` function, which was introduced in the previous notebook, as well as some utility functions ```get_parameters``` and ```set_parameters```. Feel free to revisit the code, or you can simply use the python function ```help``` (eg ```help(test)```)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9125547c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from fedlearn.model import SmallCNN, test\n",
    "from fedlearn.data_loader import load_datasets\n",
    "from fedlearn.utils import set_parameters, get_parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f9868430",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on function load_datasets in module fedlearn.data_loader:\n",
      "\n",
      "load_datasets(partition_id: int, partition_method: str, partitioner_kwargs: Dict[str, Union[str, int, float]], batch_size: int = 64, cache_dir: str = 'data', data_share_fraction: float = 0.0, data_share_seed: int = 42) -> Tuple[torch.utils.data.dataloader.DataLoader, torch.utils.data.dataloader.DataLoader, torch.utils.data.dataloader.DataLoader]\n",
      "    function for loading CIFAR-10 dataset and partitioning it for federated learning.\n",
      "    \n",
      "    Parameters:\n",
      "        partition_id:           int, the ID of the partition to load.\n",
      "        partition_method:       str, the method to use for partitioning the dataset.\n",
      "        partitioner_kwargs:     Dict[str, Union[str, int, float]], the parameters for the partitioner.\n",
      "        batch_size:             int, the size of each batch for training and validation.\n",
      "        cache_dir:              str, the directory to cache the dataset.\n",
      "        data_share_fraction:    Optional[float], fraction of the global (test) dataset to share with each client.\n",
      "        data_share_seed:        Optional[int], seed for random splitting of the dataset.\n",
      "        \n",
      "    Returns:\n",
      "        Tuple[DataLoader, DataLoader, DataLoader]: trainloader, valloader, and testloader.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(load_datasets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c3e0686c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training on cuda\n",
      "Flower 1.18.0 / PyTorch 2.7.0+cu126\n"
     ]
    }
   ],
   "source": [
    "DATADIR = Path().cwd().parent / \"data\" / \"flower_dataset\"       # specify your data directory\n",
    "\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\" # if running on mac, use: \"mps\" if torch.backends.mps.is_available() else \"cpu\"\n",
    "print(f\"Training on {DEVICE}\")\n",
    "print(f\"Flower {flwr.__version__} / PyTorch {torch.__version__}\")\n",
    "disable_progress_bar()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f28775c",
   "metadata": {},
   "source": [
    "We are now ready to specify some simulation parameters. We arbitrarily set the number of clients/partitions to 10, specify a batch size of 64, and choose the dirichlet method for partitioning the data, with $\\alpha=0.5$. For other data partitioning schemes, we encourage you to have a look at the ```flwr_datasets``` documentation on partitioners [here](https://flower.ai/docs/datasets/ref-api/flwr_datasets.partitioner.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e393fec1",
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_PARTITIONS = 10 # Number of partitions for the federated dataset same as the number of clients\n",
    "BATCH_SIZE = 32\n",
    "PARTITION_METHOD = \"dirichlet\"  # Options: \"iid\", \"dirichlet\", \"shard\"\n",
    "PARTITIONER_KWARGS = {\n",
    "    \"num_partitions\": NUM_PARTITIONS,   # Number of partitions to create\n",
    "    \"alpha\": 0.5,                       # Dirichlet parameter, only used if partition_method is \"dirichlet\"\n",
    "    \"partition_by\": \"label\"             # Partition by label, only used if partition_method is \"dirichlet\" or \"shard\"\n",
    "}\n",
    "\n",
    "# Load datasets for partition 0 to check if everything works\n",
    "_, _, _ = load_datasets(\n",
    "    partition_id=0,                         # specify partition ID to load\n",
    "    partition_method=PARTITION_METHOD,      # Method to partition the dataset\n",
    "    partitioner_kwargs=PARTITIONER_KWARGS,  # Parameters for the partitioner\n",
    "    batch_size=BATCH_SIZE,                  # Batch size for the DataLoader    \n",
    "    cache_dir=DATADIR                       # Directory to cache the datasets\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fbbce0a",
   "metadata": {},
   "source": [
    "### Define Scaffold Optimizer\n",
    "\n",
    "Recall that the local update in Scaffold is given by\n",
    "\n",
    "$$\n",
    "w^{(i)} \\gets w^{(i)} - \\eta_l \\left( g_i(w^{(i)}) + c - c_i \\right)\n",
    "$$\n",
    "\n",
    "Which can be seen as a gradient correction to Stochastic Gradient Descent (SGD). We may therefore extend the pytorch ```SGD``` class. We do this by computing the the regular SGD step, then adding the correction manually:\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "w^{(i)} &\\gets w^{(i)} - \\eta_l \\, g_i\\left(w^{(i)}\\right) \\\\\n",
    "w^{(i)} &\\gets w^{(i)} - \\eta_l (c - c_i)\n",
    "\\end{align*}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b999b922",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ScaffoldOptimizer(SGD):\n",
    "    \"\"\"\n",
    "    Extension of the SGD optimizer for the Scaffold algorithm.\n",
    "    This optimizer applies a correction term based on the global \n",
    "    and client control variables. \n",
    "    \"\"\"\n",
    "    def __init__(self, \n",
    "        params, \n",
    "        lr: float, \n",
    "        momentum: float = 0., \n",
    "        weight_decay: float = 0.\n",
    "    ) -> None: \n",
    "        super().__init__(params, lr, momentum, weight_decay)\n",
    "\n",
    "    def step_custom(\n",
    "        self, \n",
    "        global_cv: List[torch.Tensor], \n",
    "        client_cv: List[torch.Tensor]\n",
    "    ) -> None:\n",
    "        \"\"\"\n",
    "        Perform a single optimization step.\n",
    "        :param global_cv: Global control variable\n",
    "        :param client_cv: Client control variable\n",
    "        \"\"\"\n",
    "        # compute regular SGD step\n",
    "        #   w <- w - lr * grad\n",
    "        super().step() \n",
    "\n",
    "        # now add the correction term\n",
    "        #   w <- w - lr * (g_cv - c_cv)\n",
    "        device = self.param_groups[0][\"params\"][0].device\n",
    "        for group in self.param_groups:\n",
    "            for param, g_cv, c_cv in zip(group[\"params\"], global_cv, client_cv):\n",
    "                # here we add the correction term to each parameter tensor.\n",
    "                # the alpha value scales the correction term\n",
    "                    g_cv, c_cv = g_cv.to(device), c_cv.to(device)\n",
    "                    param.data.add_(g_cv - c_cv, alpha=-group[\"lr\"]) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0954175b",
   "metadata": {},
   "source": [
    "As in the previous notebook, we need to implement a ```train``` function that performs the local update for a client. As the control variates are needed for the correction step, we cannot simply reuse the function from the previous notebook. Therefore, we need to implement a similar function, that takes the control variates into account."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "862d0b68",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_scaffold(\n",
    "    net: torch.nn.Module, \n",
    "    device: torch.device, \n",
    "    trainloader: torch.utils.data.DataLoader,\n",
    "    criterion: nn.Module,\n",
    "    num_epochs: int, \n",
    "    lr: float, \n",
    "    momentum: float, \n",
    "    weight_decay: float, \n",
    "    global_cv: List[torch.Tensor], \n",
    "    client_cv: List[torch.Tensor],\n",
    ") -> None:\n",
    "    \"\"\"\n",
    "    Function that trains a model using the Scaffold optimization algorithm.\n",
    "    Parameters:\n",
    "        net:            The neural network model to train.\n",
    "        device:         The device to run the training on (CPU or GPU).\n",
    "        trainloader:    DataLoader for the training data.\n",
    "        criterion:      Loss function to use for training.\n",
    "        num_epochs:     Number of epochs to train the model.\n",
    "        lr:             Learning rate for the optimizer.\n",
    "        momentum:       Momentum factor for the optimizer.\n",
    "        weight_decay:   Weight decay (L2 penalty) for the optimizer.\n",
    "        global_cv:      Global control variables for Scaffold.\n",
    "        client_cv:      Client control variables for Scaffold.\n",
    "    \"\"\"\n",
    "    net.to(device)\n",
    "    net.train()\n",
    "\n",
    "    # initialize the custom Scaffold optimizer\n",
    "    optimizer = ScaffoldOptimizer(\n",
    "        net.parameters(), lr=lr, momentum=momentum, weight_decay=weight_decay\n",
    "    )\n",
    "    \n",
    "    for _ in range(num_epochs):\n",
    "        for i, batch in enumerate(trainloader):\n",
    "            Xtrain, Ytrain = batch[\"img\"].to(device), batch[\"label\"].to(device)\n",
    "            optimizer.zero_grad()\n",
    "            output = net(Xtrain)\n",
    "            loss = criterion(output, Ytrain)\n",
    "\n",
    "            loss.backward()\n",
    "            \n",
    "            # Perform a single optimization step with the control variables\n",
    "            optimizer.step_custom(global_cv, client_cv)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26148887",
   "metadata": {},
   "source": [
    "Now we get to the difficult part. As is the previous notebook, we need to define a custom client class for the flower framework. However, as the algorithm requires aggregation and storage of a global control variate as well, we must also implement both the aggregation class, called a ```Strategy```, as well as the server class, called a ```Server```, from scratch.\n",
    "\n",
    "We will start with the client class, as we are most familiar with this. Below are some important considerations:\n",
    "\n",
    "1. We can inherit the  ```NumPyClient``` class from the flower framework, however we need to remember to convert between ```np.ndarray``` and ```torch.tensor``` before and after local updates.\n",
    "2. There are 3 methods we need to implement, with a predefined structure\n",
    "   1. ```fit()```\n",
    "   2. ```get_parameters()```\n",
    "   3. ```evaluate()```\n",
    "\n",
    "The ```fit()``` method takes 2 parameters, ```parameters```, which in our case will be the concatenated global model parameters and the global control variate, and ```config```, which we will ignore for now. This method is responsible for all client-side updates, which means that we will call the ```train_scaffold``` function inside it to perform the local model update, as well as computing the updated local control variate. We will update the local control variate according to Option II from the original paper, which is given by:\n",
    "\n",
    "$$\n",
    "c_i^+ \\gets c_i - c + \\frac{1}{K \\eta_l}(w_t - w_{t+1}^{(i)}) \\tag{1}\n",
    "$$\n",
    "\n",
    "Where $K$ is the number of of descent steps taken, ie number of epochs times the number of mini-batches. \n",
    "\n",
    "We also need to specify what exactly is returned by the method. For clarity and in order to remain close in notation to the original paper, we will return the difference between the input model parameters and the locally updated parameters (returned from ```train_scaffold```), concatenated with the difference between the new and previous local control variate. Concretely, we return\n",
    "\n",
    "$$\n",
    "[w_{t+1}^{(i)} - w_t,\\,\\, c_i^+ - c_i]\n",
    "$$\n",
    "\n",
    "This is due to how the server-side updates look:\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "   w_{t+1} &\\gets w_t + \\frac{\\eta_g}{|\\mathcal{S}|} \\sum_{i \\in \\mathcal{S}} \\frac{n^{(i)}}{n}(w_{t+1}^{(i)} - w_t) \\tag{2.1} \\\\\n",
    "   c &\\gets c + \\frac{1}{N} \\sum_{i \\in \\mathcal{S}} \\frac{n^{(i)}}{n}(c_i^+ - c_i) \\tag{2.2}\n",
    "\\end{align*}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "a322f756",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[tensor([[0.]]), tensor([[0., 0.],\n",
      "        [0., 0.]]), tensor([[0., 0., 0.],\n",
      "        [0., 0., 0.],\n",
      "        [0., 0., 0.]])]\n"
     ]
    }
   ],
   "source": [
    "aa = [np.zeros((i, i)) for i in range(1, 4)] \n",
    "\n",
    "bb = [torch.zeros(a.shape) for a in aa]\n",
    "\n",
    "print(bb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "8f6d009c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ScaffoldClient(NumPyClient):\n",
    "    def __init__(\n",
    "        self, \n",
    "        partition_id: int, \n",
    "        net: torch.nn.Module, \n",
    "        trainloader: torch.utils.data.DataLoader, \n",
    "        valloader: torch.utils.data.DataLoader,\n",
    "        criterion: nn.Module,\n",
    "        device: torch.device,\n",
    "        num_epochs: int,\n",
    "        lr: float,\n",
    "        momentum: float,\n",
    "        weight_decay: float,\n",
    "        context: Context,\n",
    "    ) -> None:\n",
    "        self.partition_id = partition_id\n",
    "        self.net = net\n",
    "        self.trainloader = trainloader\n",
    "        self.valloader = valloader\n",
    "        self.criterion = criterion\n",
    "        self.device = device\n",
    "        self.num_epochs = num_epochs\n",
    "        self.lr = lr\n",
    "        self.momentum = momentum\n",
    "        self.weight_decay = weight_decay\n",
    "\n",
    "        self.client_state = context.state.array_records\n",
    "        self.client_cv_header = \"client_cv\"\n",
    "\n",
    "        # define directory to save client control variates\n",
    "        ##if save_dir is None:\n",
    "        ##    save_dir = \"client_cvs\"\n",
    "\n",
    "        # create directory if it does not exist\n",
    "        ##if not os.path.exists(save_dir):\n",
    "        ##    os.makedirs(save_dir)\n",
    "\n",
    "        # define the path to save the client control variates\n",
    "        ##self.save_name = os.path.join(save_dir, f\"client_{self.partition_id}_cv.pt\")\n",
    "\n",
    "        # initialize client control variates\n",
    "        ##self.client_cv = [torch.zeros(param.shape).to(torch.float32) for param in self.net.state_dict().values()]\n",
    "\n",
    "\n",
    "    # Here is where all the training logic and control variate updates happen\n",
    "    def fit(self, parameters: List[np.ndarray], config: dict) -> Tuple[List[np.ndarray], int, dict]:\n",
    "\n",
    "        # the global parameters are packed together with the global control variates\n",
    "        # in the form [params, global_cv]. we start by separating them\n",
    "        params = parameters[:len(parameters) // 2]          # list of np.ndarray\n",
    "        global_cv = parameters[len(parameters) // 2:]       # list of np.ndarray\n",
    "\n",
    "        # load the current global model:\n",
    "        set_parameters(self.net, params)\n",
    "\n",
    "        # load client control variates, if they exist:\n",
    "        if self.client_cv_header in self.client_state:\n",
    "            client_cv = self.client_state[self.client_cv_header].to_numpy_ndarrays() # list of np.ndarray\n",
    "        else:\n",
    "            # if no client control variates exist, initialize them to zero arrays\n",
    "            client_cv = [np.zeros_like(p) for p in params]  # list of np.ndarray\n",
    "\n",
    "        client_cv_torch = [torch.tensor(cv).to(torch.float32) for cv in client_cv]  # list of torch.tensor\n",
    "\n",
    "        #if os.path.exists(self.save_name):\n",
    "        #    self.client_cv = torch.load(self.save_name)     # list of torch.tensor\n",
    "\n",
    "        # convert global control variates to tensors\n",
    "        global_cv_torch = [torch.tensor(cv).to(torch.float32) for cv in global_cv]  # list of torch.tensor\n",
    "\n",
    "        # call the training function\n",
    "        train_scaffold(\n",
    "            net=self.net,\n",
    "            device=self.device,\n",
    "            trainloader=self.trainloader,\n",
    "            criterion=self.criterion,\n",
    "            num_epochs=self.num_epochs,\n",
    "            lr=self.lr,\n",
    "            momentum=self.momentum,\n",
    "            weight_decay=self.weight_decay,\n",
    "            global_cv=global_cv_torch,           # passing list of torch.tensor\n",
    "            client_cv=client_cv_torch            # passing list of torch.tensor\n",
    "        )\n",
    "\n",
    "        # update the client control variates\n",
    "        yi = get_parameters(self.net)           # list of np.ndarray\n",
    "\n",
    "        # compute coefficient for the control variates\n",
    "        # 1 / (K * eta) where K is the number of backward passes (num_epochs * len(trainloader))\n",
    "        coeff = 1. / (self.num_epochs * len(self.trainloader) * self.lr) \n",
    "\n",
    "        # define new list for udated client control variates\n",
    "        client_cv_new = []\n",
    "\n",
    "        # compute client control variate update according to eq (1), list of np.ndarray\n",
    "        for xj, yj, cj, cij in zip(params, yi, global_cv, client_cv):\n",
    "            client_cv_new.append(\n",
    "                cij - cj + coeff * (xj - yj)\n",
    "            ) \n",
    "\n",
    "        # compute server updates\n",
    "        server_update_x = [yj - xj for xj, yj in zip(params, yi)]\n",
    "        server_update_c = [cij_n - cij for cij_n, cij in zip(client_cv_new, client_cv)]\n",
    "\n",
    "        # convert client cvs back to torch tensors\n",
    "        self.client_state[self.client_cv_header] = ArrayRecord(client_cv_new)\n",
    "\n",
    "        ##self.client_cv = [torch.tensor(cv).to(torch.float32) for cv in client_cv_new]  \n",
    "\n",
    "        # save the updated client control variates\n",
    "        ##torch.save(self.client_cv, self.save_name)\n",
    "\n",
    "        #concatenate server updates\n",
    "        server_update = server_update_x + server_update_c\n",
    "\n",
    "        return server_update, len(self.trainloader.dataset), {}\n",
    "\n",
    "\n",
    "    def get_parameters(self, config: dict) -> List[np.ndarray]:\n",
    "        return get_parameters(self.net)\n",
    "\n",
    "\n",
    "    def evaluate(self, parameters: List[np.ndarray], config: dict) -> Tuple[float, int, dict]:\n",
    "        set_parameters(self.net, parameters)\n",
    "        avg_loss, accuracy = test(\n",
    "            net=self.net,\n",
    "            device=self.device,\n",
    "            testloader=self.valloader,\n",
    "            criterion=self.criterion\n",
    "        )\n",
    "        return float(avg_loss), len(self.valloader), {\"accuracy\": accuracy}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7644bf3c",
   "metadata": {},
   "source": [
    "Now that we have the flower client defined, we need to define a constructor function which the flower framework can use to instatiate clients as it goes. This is done similarly to in the previous notebook. It should be noted that all the simulation parameters, such as ```partition_method```, ```batch_size``` etc. can be passed to the ```client_fn``` function thru the context. This is useful when simulations are run from the command line. For now, we will ignore this, and hardcode it into the function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "1fc2badb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def client_fn(context: Context) -> Client:\n",
    "    partition_id = context.node_config[\"partition-id\"]\n",
    "    num_partitions = context.node_config[\"num-partitions\"]\n",
    "\n",
    "    partitioner_kwargs = {\n",
    "        \"num_partitions\": num_partitions,   # Number of partitions to create\n",
    "        \"alpha\": 0.5,                       # Dirichlet parameter\n",
    "        \"partition_by\": \"label\"             # Partition by label\n",
    "    }\n",
    "\n",
    "    trainloader, valloader, _ = load_datasets(\n",
    "        partition_id=partition_id,              # specify partition ID to load\n",
    "        partition_method=PARTITION_METHOD,      # Method to partition the dataset\n",
    "        partitioner_kwargs=partitioner_kwargs,  # Parameters for the partitioner\n",
    "        batch_size=BATCH_SIZE,                  # Batch size for the DataLoader    \n",
    "        cache_dir=DATADIR                       # Directory to cache the datasets\n",
    "    )\n",
    "\n",
    "    net = SmallCNN().to(DEVICE)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    \n",
    "    # Define hyperparameters for training\n",
    "    num_epochs = 5\n",
    "    lr = 0.01\n",
    "    momentum = 0.5\n",
    "    weight_decay = 0.\n",
    "\n",
    "    return ScaffoldClient(\n",
    "        partition_id=partition_id,\n",
    "        net=net,\n",
    "        trainloader=trainloader,\n",
    "        valloader=valloader,\n",
    "        criterion=criterion,\n",
    "        device=DEVICE,\n",
    "        num_epochs=num_epochs,\n",
    "        lr=lr,\n",
    "        momentum=momentum,\n",
    "        weight_decay=weight_decay,\n",
    "        context=context,\n",
    "        #save_dir=LOGDIR / \"client_cvs\"\n",
    "    ).to_client()\n",
    "\n",
    "\n",
    "client = ClientApp(client_fn=client_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81abe551",
   "metadata": {},
   "source": [
    "Finally, we are ready to define the server. The ```Server``` class is responsible for many things, however the main ones we are concerned with are how the initial parameters are initialised, and how the global updates are performed.\n",
    "\n",
    "We define a ```_get_initial_parameters()``` method that is essentially the same as from the base ```Server``` class, however we need to also initialise the global control variate.\n",
    "\n",
    "The ```fit_round()``` method is responsible for computing the global updates. This contains all the logic for running a single round of federated learning. The things that occur are:\n",
    "1. Send the current global model parameters and global control variate to the selected clients\n",
    "2. Collect the results/failures from the clients\n",
    "3. Aggregate the results\n",
    "4. Update the global model parameters and global control variate according to equations 2.1, 2.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "6f172f1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def concat_params(\n",
    "    parameters: Parameters, \n",
    "    global_cv: List[np.ndarray]\n",
    ") -> Parameters:\n",
    "    \"\"\"\n",
    "    Concatenate model parameters and global control variates.\n",
    "    \"\"\"\n",
    "    parameters_ndarrays = parameters_to_ndarrays(parameters)\n",
    "    parameters_ndarrays.extend(global_cv)\n",
    "    return ndarrays_to_parameters(parameters_ndarrays)\n",
    "\n",
    "\n",
    "class ScaffoldServer(Server):\n",
    "\n",
    "    def __init__(\n",
    "        self, \n",
    "        strategy: Strategy, \n",
    "        client_manager: Optional[ClientManager] = None,\n",
    "    ) -> None:\n",
    "        \n",
    "        if client_manager is None:\n",
    "            client_manager = SimpleClientManager()\n",
    "\n",
    "        super().__init__(strategy=strategy, client_manager=client_manager)\n",
    "        \n",
    "        self.global_cv: List[np.ndarray] = []  # Global control variates for Scaffold\n",
    "    \n",
    "    def _get_initial_parameters(\n",
    "        self, \n",
    "        server_round: int, \n",
    "        timeout: Optional[float]\n",
    "    ) -> Parameters: \n",
    "        \n",
    "        parameters: Optional[Parameters] = self.strategy.initialize_parameters(\n",
    "            self.client_manager\n",
    "        )\n",
    "    \n",
    "        if parameters is not None:\n",
    "            log(INFO, \"Using initial parameters provided by strategy\")\n",
    "        else:\n",
    "            # Get initial parameters from one of the clients\n",
    "            log(INFO, \"Requesting initial parameters from one random client\")\n",
    "            random_client = self._client_manager.sample(1)[0]\n",
    "            ins = GetParametersIns(config={})\n",
    "            get_parameters_res = random_client.get_parameters(\n",
    "                ins=ins, timeout=timeout, group_id=server_round\n",
    "            )\n",
    "            if get_parameters_res.status.code == Code.OK:\n",
    "                log(INFO, \"Received initial parameters from one random client\")\n",
    "            else:\n",
    "                log(\n",
    "                    WARN,\n",
    "                    \"Failed to receive initial parameters from the client.\"\n",
    "                    \" Empty initial parameters will be used.\",\n",
    "                )\n",
    "            parameters = get_parameters_res.parameters\n",
    "            \n",
    "        self.global_cv = [\n",
    "                np.zeros_like(param, dtype=np.float32) for param in parameters_to_ndarrays(parameters)\n",
    "            ]\n",
    "        \n",
    "        return parameters\n",
    "\n",
    "\n",
    "    def fit_round(\n",
    "        self,\n",
    "        server_round: int,\n",
    "        timeout: Optional[float],\n",
    "    ) -> Optional[Tuple[Optional[Parameters], Dict[str, Scalar], FitResultsAndFailures]]:\n",
    "        \n",
    "        # define client instructions to be passed to \"fit_clients\" function\n",
    "        client_instructions = self.strategy.configure_fit(\n",
    "            server_round=server_round,\n",
    "            parameters=concat_params(self.parameters, self.global_cv),  # send both model parameters and global control variates\n",
    "            client_manager=self._client_manager,\n",
    "        )\n",
    "\n",
    "        # if no clients are selected, return None\n",
    "        if not client_instructions:\n",
    "            log(INFO, f\"fit_round {server_round}: no clients selected.\")\n",
    "            return None\n",
    "        \n",
    "        log(\n",
    "            DEBUG,\n",
    "            f\"fit_round {server_round}: selected {len(client_instructions)} clients.\",\n",
    "        )\n",
    "\n",
    "        # Call the \"fit_clients\" function to perform the training on selected clients\n",
    "        results, failures = fit_clients(\n",
    "            client_instructions=client_instructions,\n",
    "            max_workers=self.max_workers,\n",
    "            timeout=timeout,\n",
    "            group_id=server_round,\n",
    "        )\n",
    "\n",
    "        log(\n",
    "            DEBUG,\n",
    "            f\"fit_round {server_round}: received {len(results)} results and {len(failures)} failures.\",\n",
    "        )\n",
    "\n",
    "        # Aggregate the results from the clients\n",
    "        aggregated_results = self.strategy.aggregate_fit(\n",
    "            server_round=server_round,\n",
    "            results=results,\n",
    "            failures=failures,\n",
    "        )\n",
    "\n",
    "        # Extract the aggregated parameters and control variates\n",
    "        aggregated_results_combined = []\n",
    "        if aggregated_results[0] is not None:\n",
    "            aggregated_results_combined = parameters_to_ndarrays(aggregated_results[0])\n",
    "\n",
    "        # Split the aggregated results into model parameters and control variates\n",
    "        aggregated_parameters = aggregated_results_combined[:len(aggregated_results_combined) // 2] # model parameters\n",
    "        aggregated_cv = aggregated_results_combined[len(aggregated_results_combined) // 2:]         # control variates\n",
    "\n",
    "        # define the update coefficient for the control variates\n",
    "        cv_coeff = len(results) / len(self._client_manager.all())\n",
    "\n",
    "        # Update the global control variates according to\n",
    "        # global_cv <- global_cv + cv_coeff * aggregated_cv\n",
    "        # where cv_coeff = |S| / N, |S| is the number of clients that participated in the round\n",
    "        # and aggregated_cv = (1 / |S|) * sum_{i in S} (c_i^+ - c_i)\n",
    "        self.global_cv = [\n",
    "            cv + cv_coeff * new_cv for cv, new_cv in zip(self.global_cv, aggregated_cv)\n",
    "        ]\n",
    "\n",
    "\n",
    "        # Update the global model parameters\n",
    "        # new_parameters = current_parameters + aggregated_parameters\n",
    "        # where current_parameters are the parameters of the global model before the round\n",
    "        # and aggregated_parameters = (1 / |S|) * sum_{i in S} (w_i^+ - w)\n",
    "        current_parameters = parameters_to_ndarrays(self.parameters)\n",
    "        new_parameters = [\n",
    "            param + update for param, update in zip(current_parameters, aggregated_parameters)\n",
    "        ]\n",
    "\n",
    "        new_parameters = ndarrays_to_parameters(new_parameters)\n",
    "\n",
    "        return new_parameters, aggregated_results[1], (results, failures)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eebdbc17",
   "metadata": {},
   "source": [
    "Again we need to create a server_fn, just like in the previous notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "139ad0e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an instance of the model and get the parameters\n",
    "params = get_parameters(SmallCNN())\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# The `evaluate` function will be called by Flower after every round\n",
    "def evaluate(\n",
    "    server_round: int,\n",
    "    parameters: list[np.ndarray],\n",
    "    config: dict[str, Scalar],\n",
    ") -> Optional[Tuple[float, dict[str, Scalar]]]:\n",
    "    \n",
    "    net = SmallCNN().to(DEVICE)\n",
    "    _, _, testloader = load_datasets(   \n",
    "        partition_id=0,                         # specify partition ID to load\n",
    "        partition_method=PARTITION_METHOD,      # Method to partition the dataset\n",
    "        partitioner_kwargs=PARTITIONER_KWARGS,  # Parameters for the partitioner\n",
    "        batch_size=BATCH_SIZE,                  # Batch size for the DataLoader    \n",
    "        cache_dir=DATADIR                       # Directory to cache the datasets\n",
    "    )\n",
    "    set_parameters(net, parameters)  # Update model with the latest parameters\n",
    "    loss, accuracy = test(\n",
    "        net=net, \n",
    "        device=DEVICE, \n",
    "        testloader=testloader, \n",
    "        criterion=criterion,\n",
    "        )\n",
    "    print(f\"Server-side evaluation loss {loss} / accuracy {accuracy}\")\n",
    "    return loss, {\"accuracy\": accuracy}\n",
    "\n",
    "\n",
    "def weighted_average(metrics: List[Tuple[int, Metrics]]) -> Metrics:\n",
    "    # Multiply accuracy of each client by number of examples used\n",
    "    accuracies = [num_examples * m[\"accuracy\"] for num_examples, m in metrics]\n",
    "    examples = [num_examples for num_examples, _ in metrics]\n",
    "\n",
    "    # Aggregate and return custom metric (weighted average)\n",
    "    return {\"accuracy\": sum(accuracies) / sum(examples)}\n",
    "\n",
    "\n",
    "def server_fn(context: Context) -> ServerAppComponents:\n",
    "    # Create FedAvg strategy\n",
    "    strategy = FedAvg(\n",
    "        fraction_fit=1.0,                                   # Use all clients for training, C\n",
    "        fraction_evaluate=0.5,                              # Use 50% of clients for evaluation\n",
    "        min_fit_clients=10,                                 # Minimum number of clients to train\n",
    "        min_evaluate_clients=5,                             # Minimum number of clients to evaluate\n",
    "        min_available_clients=NUM_PARTITIONS,               # Minimum number of clients available (enforce all clients to be available)\n",
    "        evaluate_fn=evaluate,                               # Pass the evaluation function\n",
    "        initial_parameters=ndarrays_to_parameters(params),  # Initial model parameters\n",
    "        evaluate_metrics_aggregation_fn=weighted_average,   # Custom aggregation function for evaluate metrics\n",
    "    )\n",
    "\n",
    "    server = ScaffoldServer(strategy=strategy)\n",
    "\n",
    "    # Configure the server for desired number of rounds\n",
    "    config = ServerConfig(num_rounds=2)\n",
    "    return ServerAppComponents(server=server, config=config)\n",
    "\n",
    "# Create the ServerApp\n",
    "server = ServerApp(server_fn=server_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85531a44",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92mINFO \u001b[0m:      Starting Flower ServerApp, config: num_rounds=2, no round_timeout\n",
      "\u001b[92mINFO \u001b[0m:      \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92mINFO \u001b[0m:      [INIT]\n",
      "\u001b[92mINFO \u001b[0m:      Using initial parameters provided by strategy\n",
      "\u001b[92mINFO \u001b[0m:      Starting evaluation of initial global parameters\n",
      "\u001b[92mINFO \u001b[0m:      initial parameters (loss, other metrics): 2.3021192284056933, {'accuracy': 0.099}\n",
      "\u001b[92mINFO \u001b[0m:      \n",
      "\u001b[92mINFO \u001b[0m:      [ROUND 1]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Server-side evaluation loss 2.3021192284056933 / accuracy 0.099\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[93mWARNING \u001b[0m:   No fit_metrics_aggregation_fn provided\n",
      "\u001b[92mINFO \u001b[0m:      fit progress: (1, 2.006978388030689, {'accuracy': 0.2481}, 87.7937350999564)\n",
      "\u001b[92mINFO \u001b[0m:      configure_evaluate: strategy sampled 5 clients (out of 10)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Server-side evaluation loss 2.006978388030689 / accuracy 0.2481\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92mINFO \u001b[0m:      aggregate_evaluate: received 5 results and 0 failures\n",
      "\u001b[92mINFO \u001b[0m:      \n",
      "\u001b[92mINFO \u001b[0m:      [ROUND 2]\n",
      "\u001b[92mINFO \u001b[0m:      fit progress: (2, 1.6040264360440044, {'accuracy': 0.4086}, 171.48757829982787)\n",
      "\u001b[92mINFO \u001b[0m:      configure_evaluate: strategy sampled 5 clients (out of 10)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Server-side evaluation loss 1.6040264360440044 / accuracy 0.4086\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92mINFO \u001b[0m:      aggregate_evaluate: received 5 results and 0 failures\n",
      "\u001b[92mINFO \u001b[0m:      \n",
      "\u001b[92mINFO \u001b[0m:      [SUMMARY]\n",
      "\u001b[92mINFO \u001b[0m:      Run finished 2 round(s) in 172.91s\n",
      "\u001b[92mINFO \u001b[0m:      \tHistory (loss, distributed):\n",
      "\u001b[92mINFO \u001b[0m:      \t\tround 1: 1.9975258550011967\n",
      "\u001b[92mINFO \u001b[0m:      \t\tround 2: 1.6646488485096385\n",
      "\u001b[92mINFO \u001b[0m:      \tHistory (loss, centralized):\n",
      "\u001b[92mINFO \u001b[0m:      \t\tround 0: 2.3021192284056933\n",
      "\u001b[92mINFO \u001b[0m:      \t\tround 1: 2.006978388030689\n",
      "\u001b[92mINFO \u001b[0m:      \t\tround 2: 1.6040264360440044\n",
      "\u001b[92mINFO \u001b[0m:      \tHistory (metrics, distributed, evaluate):\n",
      "\u001b[92mINFO \u001b[0m:      \t{'accuracy': [(1, 0.2255864589381928), (2, 0.3523500286068103)]}\n",
      "\u001b[92mINFO \u001b[0m:      \tHistory (metrics, centralized):\n",
      "\u001b[92mINFO \u001b[0m:      \t{'accuracy': [(0, 0.099), (1, 0.2481), (2, 0.4086)]}\n",
      "\u001b[92mINFO \u001b[0m:      \n"
     ]
    }
   ],
   "source": [
    "backend_config = {\n",
    "    #\"ray_init_args\": {\n",
    "    #    \"num_cpus\": 1,\n",
    "    #    \"num_gpus\": 1,\n",
    "    #},\n",
    "    \"client_resources\": {\n",
    "        \"num_cpus\": 2,\n",
    "        \"num_gpus\": 0.2,\n",
    "    }\n",
    "}\n",
    "run_simulation(\n",
    "    server_app=server, client_app=client, num_supernodes=NUM_PARTITIONS, backend_config=backend_config,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c24c7cf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "FLenv2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
