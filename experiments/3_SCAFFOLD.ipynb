{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "095c5c57",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import OrderedDict\n",
    "from typing import List, Tuple, Optional, Union, Dict\n",
    "import os\n",
    "from pathlib import Path\n",
    "from logging import WARNING, INFO, DEBUG\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.transforms as transforms\n",
    "from torch.optim import SGD\n",
    "from datasets.utils.logging import disable_progress_bar\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "import flwr\n",
    "from flwr.client import Client, ClientApp, NumPyClient\n",
    "from flwr.common import (\n",
    "    EvaluateIns,\n",
    "    EvaluateRes,\n",
    "    FitIns,\n",
    "    FitRes,\n",
    "    NDArrays,\n",
    "    Parameters,\n",
    "    Metrics, \n",
    "    Context, \n",
    "    Scalar, \n",
    "    ndarrays_to_parameters,\n",
    "    parameters_to_ndarrays,\n",
    "    )\n",
    "from flwr.common.logger import log\n",
    "from flwr.server import ServerApp, ServerConfig, ServerAppComponents, Server\n",
    "from flwr.server.strategy import FedAvg, Strategy\n",
    "from flwr.server.strategy.aggregate import aggregate, weighted_loss_avg\n",
    "from flwr.server.client_proxy import ClientProxy\n",
    "from flwr.server.client_manager import ClientManager, SimpleClientManager\n",
    "from flwr.simulation import run_simulation\n",
    "from flwr_datasets import FederatedDataset\n",
    "from flwr_datasets.partitioner import IidPartitioner, DirichletPartitioner, ShardPartitioner\n",
    "\n",
    "\n",
    "from fedlearn.model import SmallCNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c3e0686c",
   "metadata": {},
   "outputs": [],
   "source": [
    "datadir = Path().cwd().parent / \"data\" / \"flower_dataset\"\n",
    "logdir = Path().cwd().parent / \"logs\" / \"scaffold\"\n",
    "\n",
    "if not logdir.exists():\n",
    "    logdir.mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "002a2a64",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training on cuda\n",
      "Flower 1.18.0 / PyTorch 2.7.0+cu126\n"
     ]
    }
   ],
   "source": [
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Training on {DEVICE}\")\n",
    "print(f\"Flower {flwr.__version__} / PyTorch {torch.__version__}\")\n",
    "disable_progress_bar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e393fec1",
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_PARTITIONS = 10 # Number of partitions for the federated dataset same as the number of clients\n",
    "BATCH_SIZE = 32\n",
    "\n",
    "\n",
    "def load_datasets(partition_id: int, num_partitions: int):\n",
    "    dirichlet_partitioner = DirichletPartitioner(\n",
    "        num_partitions=num_partitions,\n",
    "        alpha=0.5,\n",
    "        partition_by=\"label\",\n",
    "    )\n",
    "    fds = FederatedDataset(dataset=\"cifar10\", partitioners={\"train\": dirichlet_partitioner}, cache_dir=datadir)\n",
    "    partition = fds.load_partition(partition_id)\n",
    "    # Divide data on each node: 80% train, 20% test\n",
    "    partition_train_test = partition.train_test_split(test_size=0.2, seed=42)\n",
    "    pytorch_transforms = transforms.Compose(\n",
    "        [transforms.ToTensor(), transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))]\n",
    "    )\n",
    "\n",
    "    def apply_transforms(batch):\n",
    "        # Instead of passing transforms to CIFAR10(..., transform=transform)\n",
    "        # we will use this function to dataset.with_transform(apply_transforms)\n",
    "        # The transforms object is exactly the same\n",
    "        batch[\"img\"] = [pytorch_transforms(img) for img in batch[\"img\"]]\n",
    "        return batch\n",
    "\n",
    "    partition_train_test = partition_train_test.with_transform(apply_transforms)\n",
    "    trainloader = DataLoader(\n",
    "        partition_train_test[\"train\"], batch_size=BATCH_SIZE, shuffle=True\n",
    "    )\n",
    "    valloader = DataLoader(partition_train_test[\"test\"], batch_size=BATCH_SIZE)\n",
    "    testset = fds.load_split(\"test\").with_transform(apply_transforms)\n",
    "    testloader = DataLoader(testset, batch_size=BATCH_SIZE)\n",
    "    return trainloader, valloader, testloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "db325f75",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Phill\\Documents\\School\\MSc\\3. Sem\\Federated Learning\\FedLearning-Final\\data\\flower_dataset\n"
     ]
    }
   ],
   "source": [
    "print(datadir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "18de45e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Phill\\anaconda3\\envs\\FLenv2\\Lib\\site-packages\\flwr_datasets\\utils.py:109: UserWarning: The currently tested dataset are ['mnist', 'ylecun/mnist', 'cifar10', 'uoft-cs/cifar10', 'fashion_mnist', 'zalando-datasets/fashion_mnist', 'sasha/dog-food', 'zh-plus/tiny-imagenet', 'scikit-learn/adult-census-income', 'cifar100', 'uoft-cs/cifar100', 'svhn', 'ufldl-stanford/svhn', 'sentiment140', 'stanfordnlp/sentiment140', 'speech_commands', 'LIUM/tedlium', 'flwrlabs/femnist', 'flwrlabs/ucf101', 'flwrlabs/ambient-acoustic-context', 'jlh/uci-mushrooms', 'Mike0307/MNIST-M', 'flwrlabs/usps', 'scikit-learn/iris', 'flwrlabs/pacs', 'flwrlabs/cinic10', 'flwrlabs/caltech101', 'flwrlabs/office-home', 'flwrlabs/fed-isic2019']. Given: c:\\Users\\Phill\\Documents\\School\\MSc\\3. Sem\\Federated Learning\\FedLearning-Final\\data\\flower_dataset.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "Path.replace() takes 2 positional arguments but 3 were given",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[7]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m _, _, _ = \u001b[43mload_datasets\u001b[49m\u001b[43m(\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mNUM_PARTITIONS\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Load datasets for partition 0 to check if everything works\u001b[39;00m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 13\u001b[39m, in \u001b[36mload_datasets\u001b[39m\u001b[34m(partition_id, num_partitions)\u001b[39m\n\u001b[32m     11\u001b[39m \u001b[38;5;66;03m#fds = FederatedDataset(dataset=\"cifar10\", partitioners={\"train\": dirichlet_partitioner}, cache_dir=datadir)\u001b[39;00m\n\u001b[32m     12\u001b[39m fds = FederatedDataset(dataset=datadir, partitioners={\u001b[33m\"\u001b[39m\u001b[33mtrain\u001b[39m\u001b[33m\"\u001b[39m: dirichlet_partitioner})\n\u001b[32m---> \u001b[39m\u001b[32m13\u001b[39m partition = \u001b[43mfds\u001b[49m\u001b[43m.\u001b[49m\u001b[43mload_partition\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpartition_id\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     14\u001b[39m \u001b[38;5;66;03m# Divide data on each node: 80% train, 20% test\u001b[39;00m\n\u001b[32m     15\u001b[39m partition_train_test = partition.train_test_split(test_size=\u001b[32m0.2\u001b[39m, seed=\u001b[32m42\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Phill\\anaconda3\\envs\\FLenv2\\Lib\\site-packages\\flwr_datasets\\federated_dataset.py:177\u001b[39m, in \u001b[36mFederatedDataset.load_partition\u001b[39m\u001b[34m(self, partition_id, split)\u001b[39m\n\u001b[32m    149\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Load the partition specified by the idx in the selected split.\u001b[39;00m\n\u001b[32m    150\u001b[39m \n\u001b[32m    151\u001b[39m \u001b[33;03mThe dataset is downloaded only when the first call to `load_partition` or\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    174\u001b[39m \u001b[33;03m    Single partition from the dataset split.\u001b[39;00m\n\u001b[32m    175\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    176\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m._dataset_prepared:\n\u001b[32m--> \u001b[39m\u001b[32m177\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_prepare_dataset\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    178\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._dataset \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    179\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mDataset is not loaded yet.\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Phill\\anaconda3\\envs\\FLenv2\\Lib\\site-packages\\flwr_datasets\\federated_dataset.py:314\u001b[39m, in \u001b[36mFederatedDataset._prepare_dataset\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    293\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_prepare_dataset\u001b[39m(\u001b[38;5;28mself\u001b[39m) -> \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    294\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Prepare the dataset (prior to partitioning) by download, shuffle, replit.\u001b[39;00m\n\u001b[32m    295\u001b[39m \n\u001b[32m    296\u001b[39m \u001b[33;03m    Run only ONCE when triggered by load_* function. (In future more control whether\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    312\u001b[39m \u001b[33;03m    happen before the resplitting.\u001b[39;00m\n\u001b[32m    313\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m314\u001b[39m     \u001b[38;5;28mself\u001b[39m._dataset = \u001b[43mdatasets\u001b[49m\u001b[43m.\u001b[49m\u001b[43mload_dataset\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    315\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpath\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_dataset_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_subset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_load_dataset_kwargs\u001b[49m\n\u001b[32m    316\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    317\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m._dataset, datasets.DatasetDict):\n\u001b[32m    318\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m    319\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mProbably one of the specified parameter in `load_dataset_kwargs` \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    320\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mchange the return type of the datasets.load_dataset function. \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    321\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mMake sure to use parameter such that the return type is DatasetDict. \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    322\u001b[39m             \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mThe return type is currently: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28mself\u001b[39m._dataset)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    323\u001b[39m         )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Phill\\anaconda3\\envs\\FLenv2\\Lib\\site-packages\\datasets\\load.py:2132\u001b[39m, in \u001b[36mload_dataset\u001b[39m\u001b[34m(path, name, data_dir, data_files, split, cache_dir, features, download_config, download_mode, verification_mode, keep_in_memory, save_infos, revision, token, streaming, num_proc, storage_options, trust_remote_code, **config_kwargs)\u001b[39m\n\u001b[32m   2127\u001b[39m verification_mode = VerificationMode(\n\u001b[32m   2128\u001b[39m     (verification_mode \u001b[38;5;129;01mor\u001b[39;00m VerificationMode.BASIC_CHECKS) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m save_infos \u001b[38;5;28;01melse\u001b[39;00m VerificationMode.ALL_CHECKS\n\u001b[32m   2129\u001b[39m )\n\u001b[32m   2131\u001b[39m \u001b[38;5;66;03m# Create a dataset builder\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m2132\u001b[39m builder_instance = \u001b[43mload_dataset_builder\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2133\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpath\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2134\u001b[39m \u001b[43m    \u001b[49m\u001b[43mname\u001b[49m\u001b[43m=\u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2135\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdata_dir\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdata_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2136\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdata_files\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdata_files\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2137\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2138\u001b[39m \u001b[43m    \u001b[49m\u001b[43mfeatures\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfeatures\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2139\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdownload_config\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdownload_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2140\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdownload_mode\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdownload_mode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2141\u001b[39m \u001b[43m    \u001b[49m\u001b[43mrevision\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2142\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2143\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2144\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtrust_remote_code\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtrust_remote_code\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2145\u001b[39m \u001b[43m    \u001b[49m\u001b[43m_require_default_config_name\u001b[49m\u001b[43m=\u001b[49m\u001b[43mname\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m   2146\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mconfig_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2147\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2149\u001b[39m \u001b[38;5;66;03m# Return iterable dataset in case of streaming\u001b[39;00m\n\u001b[32m   2150\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m streaming:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Phill\\anaconda3\\envs\\FLenv2\\Lib\\site-packages\\datasets\\load.py:1853\u001b[39m, in \u001b[36mload_dataset_builder\u001b[39m\u001b[34m(path, name, data_dir, data_files, cache_dir, features, download_config, download_mode, revision, token, storage_options, trust_remote_code, _require_default_config_name, **config_kwargs)\u001b[39m\n\u001b[32m   1851\u001b[39m     download_config = download_config.copy() \u001b[38;5;28;01mif\u001b[39;00m download_config \u001b[38;5;28;01melse\u001b[39;00m DownloadConfig()\n\u001b[32m   1852\u001b[39m     download_config.storage_options.update(storage_options)\n\u001b[32m-> \u001b[39m\u001b[32m1853\u001b[39m dataset_module = \u001b[43mdataset_module_factory\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1854\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1855\u001b[39m \u001b[43m    \u001b[49m\u001b[43mrevision\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1856\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdownload_config\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdownload_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1857\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdownload_mode\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdownload_mode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1858\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdata_dir\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdata_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1859\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdata_files\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdata_files\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1860\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1861\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtrust_remote_code\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtrust_remote_code\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1862\u001b[39m \u001b[43m    \u001b[49m\u001b[43m_require_default_config_name\u001b[49m\u001b[43m=\u001b[49m\u001b[43m_require_default_config_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1863\u001b[39m \u001b[43m    \u001b[49m\u001b[43m_require_custom_configs\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mbool\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mconfig_kwargs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1864\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1865\u001b[39m \u001b[38;5;66;03m# Get dataset builder class from the processing script\u001b[39;00m\n\u001b[32m   1866\u001b[39m builder_kwargs = dataset_module.builder_kwargs\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Phill\\anaconda3\\envs\\FLenv2\\Lib\\site-packages\\datasets\\load.py:1534\u001b[39m, in \u001b[36mdataset_module_factory\u001b[39m\u001b[34m(path, revision, download_config, download_mode, dynamic_modules_path, data_dir, data_files, cache_dir, trust_remote_code, _require_default_config_name, _require_custom_configs, **download_kwargs)\u001b[39m\n\u001b[32m   1531\u001b[39m download_config.force_extract = \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m   1532\u001b[39m download_config.force_download = download_mode == DownloadMode.FORCE_REDOWNLOAD\n\u001b[32m-> \u001b[39m\u001b[32m1534\u001b[39m filename = \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mfilter\u001b[39m(\u001b[38;5;28;01mlambda\u001b[39;00m x: x, \u001b[43mpath\u001b[49m\u001b[43m.\u001b[49m\u001b[43mreplace\u001b[49m\u001b[43m(\u001b[49m\u001b[43mos\u001b[49m\u001b[43m.\u001b[49m\u001b[43msep\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m/\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m.split(\u001b[33m\"\u001b[39m\u001b[33m/\u001b[39m\u001b[33m\"\u001b[39m)))[-\u001b[32m1\u001b[39m]\n\u001b[32m   1535\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m filename.endswith(\u001b[33m\"\u001b[39m\u001b[33m.py\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m   1536\u001b[39m     filename = filename + \u001b[33m\"\u001b[39m\u001b[33m.py\u001b[39m\u001b[33m\"\u001b[39m\n",
      "\u001b[31mTypeError\u001b[39m: Path.replace() takes 2 positional arguments but 3 were given"
     ]
    }
   ],
   "source": [
    "_, _, _ = load_datasets(0, NUM_PARTITIONS)  # Load datasets for partition 0 to check if everything works"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fbbce0a",
   "metadata": {},
   "source": [
    "### Define Scaffold Optimizer\n",
    "\n",
    "Recall that the local update in Scaffold is given by\n",
    "\n",
    "$$\n",
    "w^{(i)} \\gets w^{(i)} - \\eta_l \\left( g_i(w^{(i)}) + c - c_i \\right)\n",
    "$$\n",
    "\n",
    "Which can be seen as a gradient correction to Stochastic Gradient Descent (SGD). We may therefore extend the pytorch ```SGD``` class. We do this by computing the the regular SGD step, then adding the correction manually:\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "w^{(i)} &\\gets w^{(i)} - \\eta_l \\, g_i\\left(w^{(i)}\\right) \\\\\n",
    "w^{(i)} &\\gets w^{(i)} - \\eta_l (c - c_i)\n",
    "\\end{align*}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b999b922",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ScaffoldOptimizer(SGD):\n",
    "    def __init__(self, params, lr, momentum=0., weight_decay=0.):\n",
    "        super().__init__(params, lr, momentum, weight_decay)\n",
    "\n",
    "    def step_custom(self, global_cv, client_cv):\n",
    "        \"\"\"\n",
    "        Perform a single optimization step.\n",
    "        :param global_cv: Global control variable\n",
    "        :param client_cv: Client control variable\n",
    "        \"\"\"\n",
    "        # compute regular SGD step\n",
    "        #   w <- w - lr * grad\n",
    "        super().step() \n",
    "\n",
    "        # now add the correction term\n",
    "        #   w <- w - lr * (g_cv - c_cv)\n",
    "        device = self.param_groups[0][\"params\"][0].device\n",
    "        for group in self.param_groups:\n",
    "            for param, g_cv, c_cv in zip(group[\"params\"], global_cv, client_cv):\n",
    "                # here we add the correction term to each parameter tensor.\n",
    "                # the alpha value scales the correction term\n",
    "                    g_cv, c_cv = g_cv.to(device), c_cv.to(device)\n",
    "                    param.data.add_(g_cv - c_cv, alpha=-group[\"lr\"]) \n",
    "                #if param.grad is not None:\n",
    "                    #g_cv, c_cv = g_cv.to(device), c_cv.to(device)\n",
    "                    #param.grad.add_(g_cv - c_cv)  #, alpha=-group[\"lr\"]) \n",
    "        #super().step()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0954175b",
   "metadata": {},
   "source": [
    "We can now write a function for the local training. In this function, we want simply want to perform gradient corrected SGD updates over the local data for $E$ epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "862d0b68",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_scaffold(net: torch.nn.Module, \n",
    "                   device: torch.device, \n",
    "                   trainloader: torch.utils.data.DataLoader,\n",
    "                   criterion: nn.Module,\n",
    "                   num_epochs: int, \n",
    "                   lr: float, \n",
    "                   momentum: float, \n",
    "                   weight_decay: float, \n",
    "                   global_cv: List[torch.Tensor], \n",
    "                   client_cv: List[torch.Tensor],\n",
    "                   ) -> None:\n",
    "    \"\"\"\n",
    "    Function that trains a model using the Scaffold optimization algorithm.\n",
    "    Parameters:\n",
    "        net:            The neural network model to train.\n",
    "        device:         The device to run the training on (CPU or GPU).\n",
    "        trainloader:    DataLoader for the training data.\n",
    "        criterion:      Loss function to use for training.\n",
    "        num_epochs:     Number of epochs to train the model.\n",
    "        lr:             Learning rate for the optimizer.\n",
    "        momentum:       Momentum factor for the optimizer.\n",
    "        weight_decay:   Weight decay (L2 penalty) for the optimizer.\n",
    "        global_cv:      Global control variables for Scaffold.\n",
    "        client_cv:      Client control variables for Scaffold.\n",
    "    \"\"\"\n",
    "    net.to(device)\n",
    "    net.train()\n",
    "    optimizer = ScaffoldOptimizer(\n",
    "        net.parameters(), lr=lr, momentum=momentum, weight_decay=weight_decay\n",
    "    )\n",
    "    \n",
    "    for _ in range(num_epochs):\n",
    "        for batch in trainloader:\n",
    "            Xtrain, Ytrain = batch[\"img\"].to(device), batch[\"label\"].to(device)\n",
    "            optimizer.zero_grad()\n",
    "            output = net(Xtrain)\n",
    "            loss = criterion(output, Ytrain)\n",
    "\n",
    "            # for debugging purposes, exit if loss is NaN\n",
    "            #if torch.isnan(loss):\n",
    "            #    raise ValueError(\"Loss is NaN, check your model and data.\")\n",
    "\n",
    "            loss.backward()\n",
    "            \n",
    "            # Perform a single optimization step with the control variables\n",
    "            optimizer.step_custom(global_cv, client_cv)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "436bd857",
   "metadata": {},
   "source": [
    "We will also define a test function, which will be called to evaluate our model. This will give us some metrics to evaluate the performance of the model. As we are working with a classifier, we are interested in both the loss and accuracy of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8975daea",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(net: torch.nn.Module, \n",
    "         device: torch.device, \n",
    "         testloader: torch.utils.data.DataLoader,\n",
    "         criterion: nn.Module,\n",
    "         ) -> Tuple[float, float]:\n",
    "    \"\"\"\n",
    "    Function that tests a model on the test dataset.\n",
    "    Parameters:\n",
    "        net:        The neural network model to test.\n",
    "        device:     The device to run the testing on (CPU or GPU).\n",
    "        testloader: DataLoader for the test data.\n",
    "        criterion:  Loss function to use for testing.\n",
    "    Returns:\n",
    "        Tuple containing the average loss and accuracy on the test set.\n",
    "    \"\"\"\n",
    "    net.to(device)\n",
    "    net.eval()\n",
    "    total_loss = 0.0    # Accumulator for total loss\n",
    "    correct = 0         # tracker for correct predictions\n",
    "    total = 0           # tracker for total predictions\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in testloader:\n",
    "            Xtest, Ytest = batch[\"img\"].to(device), batch[\"label\"].to(device)\n",
    "            output = net(Xtest)\n",
    "            loss = criterion(output, Ytest)\n",
    "\n",
    "            if torch.isnan(loss):\n",
    "                raise ValueError(\"Loss is NaN, check your model and data.\")\n",
    "\n",
    "            total_loss += loss.item()\n",
    "            _, predicted = output.max(1)\n",
    "            total += Ytest.size(0)\n",
    "            correct += predicted.eq(Ytest).sum().item()\n",
    "    \n",
    "    avg_loss = total_loss / len(testloader) # compute the average loss\n",
    "    accuracy = correct / total              # compute the accuracy\n",
    "    return avg_loss, accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26148887",
   "metadata": {},
   "source": [
    "For the simulation, we will use the flower framework, which was introduced in the _ notebook. To do so, we need to specify both Client and Server classes. We need to consider a couple of things: \n",
    "1. We can inherit the  ```NumPyClient``` class from the flower framework, however we need to remember to convert between ```np.ndarray``` and ```torch.tensor``` before and after local updates.\n",
    "2. We need to specify a ```client.fit()``` method, containing all the logic for the local update. This method has 2 inputs\n",
    "   1. parameters: a list of ```np.ndarray```, containing both global model parameters and global control variates\n",
    "   2. config: dict for specifying training configuration (we will ignore this for now)\n",
    "3. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8f6d009c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define helper functions to set and get model parameters\n",
    "def get_parameters(net) -> List[np.ndarray]:\n",
    "    return [val.cpu().numpy() for _, val in net.state_dict().items()]\n",
    "\n",
    "def set_parameters(net: torch.nn.Module, parameters: List[np.ndarray]) -> None:\n",
    "    params_dict = zip(net.state_dict().keys(), parameters)\n",
    "    state_dict = OrderedDict({k : torch.tensor(v).to(torch.float32) for k, v in params_dict})\n",
    "    net.load_state_dict(state_dict, strict=True)\n",
    "\n",
    "\n",
    "class ScaffoldClient(NumPyClient):\n",
    "    def __init__(self, \n",
    "                 partition_id: int, \n",
    "                 net: torch.nn.Module, \n",
    "                 trainloader: torch.utils.data.DataLoader, \n",
    "                 valloader: torch.utils.data.DataLoader,\n",
    "                 criterion: nn.Module,\n",
    "                 device: torch.device,\n",
    "                 num_epochs: int,\n",
    "                 lr: float,\n",
    "                 momentum: float,\n",
    "                 weight_decay: float,\n",
    "                 save_dir: Optional[str] = None,\n",
    "                 ):\n",
    "        self.partition_id = partition_id\n",
    "        self.net = net\n",
    "        self.trainloader = trainloader\n",
    "        self.valloader = valloader\n",
    "        self.criterion = criterion\n",
    "        self.device = device\n",
    "        self.num_epochs = num_epochs\n",
    "        self.lr = lr\n",
    "        self.momentum = momentum\n",
    "        self.weight_decay = weight_decay\n",
    "\n",
    "        # define directory to save client control variates\n",
    "        if save_dir is None:\n",
    "            save_dir = \"client_cvs\"\n",
    "\n",
    "        # create directory if it does not exist\n",
    "        if not os.path.exists(save_dir):\n",
    "            os.makedirs(save_dir)\n",
    "\n",
    "        # define the path to save the client control variates\n",
    "        self.save_name = os.path.join(save_dir, f\"client_{self.partition_id}_cv.pt\")\n",
    "\n",
    "        # initialize client control variates\n",
    "        self.client_cv = [torch.zeros(param.shape).to(torch.float32) for param in self.net.state_dict().values()]\n",
    "\n",
    "\n",
    "    # Here is where all the training logic and control variate updates happen\n",
    "    def fit(self, parameters: List[np.ndarray], config: dict) -> Tuple[List[np.ndarray], int, dict]:\n",
    "\n",
    "        # the global parameters are packed together with the global control variates\n",
    "        # in the form [params, global_cv]. we start by separating them\n",
    "        params = parameters[:len(parameters) // 2]          # list of np.ndarray\n",
    "        global_cv = parameters[len(parameters) // 2:]       # list of np.ndarray\n",
    "\n",
    "        # load the current global model:\n",
    "        set_parameters(self.net, params)\n",
    "\n",
    "        # load client control variates, if they exist:\n",
    "        if os.path.exists(self.save_name):\n",
    "            self.client_cv = torch.load(self.save_name)     # list of torch.tensor\n",
    "\n",
    "        # convert global control variates to tensors\n",
    "        global_cv_torch = [torch.tensor(cv).to(torch.float32) for cv in global_cv]  # list of torch.tensor\n",
    "\n",
    "        # call the training function\n",
    "        train_scaffold(\n",
    "            net=self.net,\n",
    "            device=self.device,\n",
    "            trainloader=self.trainloader,\n",
    "            criterion=self.criterion,\n",
    "            num_epochs=self.num_epochs,\n",
    "            lr=self.lr,\n",
    "            momentum=self.momentum,\n",
    "            weight_decay=self.weight_decay,\n",
    "            global_cv=global_cv_torch,          # passing list of torch.tensor\n",
    "            client_cv=self.client_cv            # passing list of torch.tensor\n",
    "        )\n",
    "\n",
    "        # update the client control variates\n",
    "        yi = get_parameters(self.net)           # list of np.ndarray\n",
    "\n",
    "        # compute coefficient for the control variates\n",
    "        # 1 / (K * eta) where K is the number of backward passes (num_epochs * len(trainloader))\n",
    "        coeff = 1. / (self.num_epochs * len(self.trainloader) * self.lr) \n",
    "\n",
    "        client_cv = [cv.numpy() for cv in self.client_cv]  # list of np.ndarray\n",
    "\n",
    "        # define new list for udated client control variates\n",
    "        client_cv_new = []\n",
    "\n",
    "        # compute client control variate update, list of np.ndarray\n",
    "        for xj, yj, cj, cij in zip(params, yi, global_cv, client_cv):\n",
    "            client_cv_new.append(\n",
    "                cij - cj + coeff * (xj - yj)\n",
    "            ) \n",
    "\n",
    "        # compute server updates\n",
    "        server_update_x = [yj - xj for xj, yj in zip(params, yi)]\n",
    "        server_update_c = [cij_n - cij for cij_n, cij in zip(client_cv_new, client_cv)]\n",
    "\n",
    "        # convert client cvs back to torch tensors\n",
    "        self.client_cv = [torch.tensor(cv).to(torch.float32) for cv in client_cv_new]  \n",
    "\n",
    "        # save the updated client control variates\n",
    "        torch.save(self.client_cv, self.save_name)\n",
    "\n",
    "        #concatenate server updates\n",
    "        server_update = server_update_x + server_update_c\n",
    "\n",
    "        return server_update, len(self.trainloader.dataset), {}\n",
    "\n",
    "\n",
    "\n",
    "    def evaluate(self, parameters: List[np.ndarray], config: dict) -> Tuple[float, int, dict]:\n",
    "        set_parameters(self.net, parameters)\n",
    "        avg_loss, accuracy = test(\n",
    "            net=self.net,\n",
    "            device=self.device,\n",
    "            testloader=self.valloader,\n",
    "            criterion=self.criterion\n",
    "        )\n",
    "        return float(avg_loss), len(self.valloader), {\"accuracy\": accuracy}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7644bf3c",
   "metadata": {},
   "source": [
    "Now that we have the flower client defined, we need to define a constructor function which the flower framework can use to instatiate clients as it goes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1fc2badb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def client_fn(context: Context) -> Client:\n",
    "    partition_id = context.node_config[\"partition-id\"]\n",
    "    num_partitions = context.node_config[\"num-partitions\"]\n",
    "    trainloader, valloader, _ = load_datasets(partition_id, num_partitions)\n",
    "\n",
    "    net = SmallCNN().to(DEVICE)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    \n",
    "    # Define hyperparameters for training\n",
    "    num_epochs = 5\n",
    "    lr = 1e-2\n",
    "    momentum = 0.\n",
    "    weight_decay = 0.\n",
    "\n",
    "    return ScaffoldClient(\n",
    "        partition_id=partition_id,\n",
    "        net=net,\n",
    "        trainloader=trainloader,\n",
    "        valloader=valloader,\n",
    "        criterion=criterion,\n",
    "        device=DEVICE,\n",
    "        num_epochs=num_epochs,\n",
    "        lr=lr,\n",
    "        momentum=momentum,\n",
    "        weight_decay=weight_decay,\n",
    "        save_dir=logdir / \"client_cvs\"\n",
    "    ).to_client()\n",
    "\n",
    "\n",
    "client = ClientApp(client_fn=client_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "661d41df",
   "metadata": {},
   "source": [
    "We also need to implement a custom strategy. We can inherit the ```FedAvg``` class. All we need to do is redefine the ```aggregate_fit()``` method. This method must take the following as input:\n",
    "1. server_round: the current round \n",
    "2. results:\n",
    "3. failures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "53151c90",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ScaffoldStrategy(Strategy):\n",
    "    def __init__(\n",
    "        self,\n",
    "        total_num_clients: int,\n",
    "        fraction_fit: float = 1.0,\n",
    "        fraction_evaluate: float = 1.0,\n",
    "        min_fit_clients: int = 2,\n",
    "        min_evaluate_clients: int = 2,\n",
    "        min_available_clients: int = 2,\n",
    "        evaluate_fn: Optional[callable] = None,\n",
    "        accept_failures: bool = False,\n",
    "        fit_metrics_aggregation_fn: Optional[callable] = None,\n",
    "    ) -> None:\n",
    "        super().__init__()\n",
    "        self.total_num_clients = total_num_clients\n",
    "        total_num_clients = total_num_clients\n",
    "        self.fraction_fit = fraction_fit\n",
    "        self.fraction_evaluate = fraction_evaluate\n",
    "        self.min_fit_clients = min_fit_clients\n",
    "        self.min_evaluate_clients = min_evaluate_clients\n",
    "        self.min_available_clients = min_available_clients\n",
    "        self.accept_failures = accept_failures\n",
    "        self.fit_metrics_aggregation_fn = fit_metrics_aggregation_fn\n",
    "\n",
    "        self.evaluate_fn = evaluate_fn\n",
    "\n",
    "\n",
    "    def __repr__(self) -> str:\n",
    "        return \"ScaffoldStrategy\"\n",
    "\n",
    "\n",
    "    def initialize_parameters(\n",
    "        self, client_manager: ClientManager\n",
    "    ) -> Optional[Parameters]:\n",
    "        \"\"\"Initialize global model parameters.\"\"\"\n",
    "        net = SmallCNN()\n",
    "        parameters = get_parameters(net)        \n",
    "        return ndarrays_to_parameters(parameters)\n",
    "\n",
    "\n",
    "    def configure_fit(\n",
    "        self, server_round: int, parameters: Parameters, client_manager: ClientManager\n",
    "    ) -> List[Tuple[ClientProxy, FitIns]]:\n",
    "        \"\"\"Configure the next round of training.\"\"\"\n",
    "\n",
    "        config = {}\n",
    "        fit_ins = FitIns(parameters, config)\n",
    "\n",
    "        # Sample clients\n",
    "        sample_size, min_num_clients = self.num_fit_clients(\n",
    "            client_manager.num_available()\n",
    "        )\n",
    "        clients = client_manager.sample(\n",
    "            num_clients=sample_size, min_num_clients=min_num_clients\n",
    "        )\n",
    "\n",
    "        fit_configurations = [(client, fit_ins) for client in clients]\n",
    "        \n",
    "        return fit_configurations\n",
    "\n",
    "\n",
    "    def aggregate_fit(self, \n",
    "                      server_round: int, \n",
    "                      results: List[Tuple[ClientProxy, FitRes]],\n",
    "                      failures: List[Union[Tuple[ClientProxy, FitRes], BaseException]],\n",
    "                      ) -> Tuple[Optional[Parameters], Dict[str, Scalar]]:\n",
    "        \"\"\"\n",
    "        aggregation method for Scaffold strategy.\n",
    "        \"\"\"\n",
    "        if not results:\n",
    "            return None, {}\n",
    "        if not self.accept_failures and failures:\n",
    "            return None, {}\n",
    "        \n",
    "        combined_parameters = [\n",
    "            parameters_to_ndarrays(fit_res.parameters) for _, fit_res in results\n",
    "        ]\n",
    "\n",
    "        len_combined_parameters = len(combined_parameters[0]) # combined number of model parameters and control variates\n",
    "\n",
    "        num_samples_all = [fit_res.num_examples for _, fit_res in results]  # number of training samples from each client\n",
    "\n",
    "        # The \"aggregate()\" function expects a list of tuples, where each tuple contains\n",
    "        # the local parameters and the number of samples for that client.\n",
    "        aggregation_inputs_parameters = [\n",
    "            (local_params[:len_combined_parameters // 2], num_samples) \n",
    "            for local_params, num_samples in zip(combined_parameters, num_samples_all)\n",
    "        ]\n",
    "        \n",
    "        parameters_aggregated = aggregate(aggregation_inputs_parameters)\n",
    "\n",
    "        aggregation_inputs_cv = [\n",
    "            (local_params[len_combined_parameters // 2:], num_samples) \n",
    "            for local_params, num_samples in zip(combined_parameters, num_samples_all)\n",
    "        ]\n",
    "\n",
    "        cv_aggregated = aggregate(aggregation_inputs_cv)\n",
    "\n",
    "        metrics_aggregated = {}\n",
    "        if self.fit_metrics_aggregation_fn is not None:\n",
    "            fit_metrics = [\n",
    "                (fit_res.num_examples, fit_res.metrics)\n",
    "                for _, fit_res in results\n",
    "            ]\n",
    "            metrics_aggregated = self.fit_metrics_aggregation_fn(fit_metrics)\n",
    "        elif server_round == 1:\n",
    "            log(WARNING, \"No fit_metrics_aggregation_fn provided\")\n",
    "        \n",
    "\n",
    "        return (\n",
    "            ndarrays_to_parameters(parameters_aggregated + cv_aggregated),\n",
    "            metrics_aggregated,\n",
    "        )\n",
    "\n",
    "\n",
    "    def configure_evaluate(\n",
    "        self, server_round: int, parameters: Parameters, client_manager: ClientManager\n",
    "    ) -> List[Tuple[ClientProxy, EvaluateIns]]:\n",
    "        \"\"\"Configure the next round of evaluation.\"\"\"\n",
    "        if self.fraction_evaluate == 0.0:\n",
    "            return []\n",
    "        config = {}\n",
    "        evaluate_ins = EvaluateIns(parameters, config)\n",
    "\n",
    "        # Sample clients\n",
    "        sample_size, min_num_clients = self.num_evaluation_clients(\n",
    "            client_manager.num_available()\n",
    "        )\n",
    "        clients = client_manager.sample(\n",
    "            num_clients=sample_size, min_num_clients=min_num_clients\n",
    "        )\n",
    "\n",
    "        # Return client/config pairs\n",
    "        return [(client, evaluate_ins) for client in clients]\n",
    "\n",
    "\n",
    "    def aggregate_evaluate(\n",
    "        self,\n",
    "        server_round: int,\n",
    "        results: List[Tuple[ClientProxy, EvaluateRes]],\n",
    "        failures: List[Union[Tuple[ClientProxy, EvaluateRes], BaseException]],\n",
    "    ) -> Tuple[Optional[float], Dict[str, Scalar]]:\n",
    "        \"\"\"Aggregate evaluation losses using weighted average.\"\"\"\n",
    "\n",
    "        if not results:\n",
    "            return None, {}\n",
    "\n",
    "        loss_aggregated = weighted_loss_avg(\n",
    "            [\n",
    "                (evaluate_res.num_examples, evaluate_res.loss)\n",
    "                for _, evaluate_res in results\n",
    "            ]\n",
    "        )\n",
    "        metrics_aggregated = {}\n",
    "        return loss_aggregated, metrics_aggregated\n",
    "\n",
    "\n",
    "    # method for evaluating the global model\n",
    "    def evaluate(\n",
    "        self, server_round: int, parameters: Parameters\n",
    "    ) -> Optional[Tuple[float, Dict[str, Scalar]]]:\n",
    "        \"\"\"Evaluate global model parameters using an evaluation function.\"\"\"\n",
    "        if self.evaluate_fn is None:\n",
    "            return None\n",
    "            # If an evaluation function is provided, use it\n",
    "        parameters_ndarray = parameters_to_ndarrays(parameters)\n",
    "        eval_res = self.evaluate_fn(server_round, parameters_ndarray, {})\n",
    "        if eval_res is None:\n",
    "            return None\n",
    "        loss, metrics = eval_res\n",
    "        return loss, metrics\n",
    "\n",
    "\n",
    "    # boilerplate code\n",
    "    def num_fit_clients(self, num_available_clients: int) -> Tuple[int, int]:\n",
    "        \"\"\"Return sample size and required number of clients.\"\"\"\n",
    "        num_clients = int(num_available_clients * self.fraction_fit)\n",
    "        return max(num_clients, self.min_fit_clients), self.min_available_clients\n",
    "\n",
    "\n",
    "    # boilerplate code\n",
    "    def num_evaluation_clients(self, num_available_clients: int) -> Tuple[int, int]:\n",
    "        \"\"\"Use a fraction of available clients for evaluation.\"\"\"\n",
    "        num_clients = int(num_available_clients * self.fraction_evaluate)\n",
    "        return max(num_clients, self.min_evaluate_clients), self.min_available_clients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6f172f1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from flwr.server.server import FitResultsAndFailures, fit_clients\n",
    "\n",
    "def concat_params(parameters: Parameters, global_cv: List[np.ndarray]) -> Parameters:\n",
    "    \"\"\"\n",
    "    Concatenate model parameters and global control variates.\n",
    "    \"\"\"\n",
    "    parameters_ndarrays = parameters_to_ndarrays(parameters)\n",
    "    parameters_ndarrays.extend(global_cv)\n",
    "    return ndarrays_to_parameters(parameters_ndarrays)\n",
    "\n",
    "class ScaffoldServer(Server):\n",
    "\n",
    "    def __init__(self, \n",
    "                 strategy: Strategy, \n",
    "                 client_manager: ClientManager = SimpleClientManager(),\n",
    "                 ) -> None:\n",
    "        super().__init__(strategy=strategy, client_manager=client_manager)\n",
    "        \n",
    "        self.global_cv: List[np.ndarray] = []  # Global control variates for Scaffold\n",
    "    \n",
    "    def _get_initial_parameters(\n",
    "        self, server_round: int, timeout: Optional[float]\n",
    "        ) -> Parameters: \n",
    "        \n",
    "        parameters = self.strategy.initialize_parameters(self.client_manager)\n",
    "\n",
    "        if parameters is not None:\n",
    "            log(INFO, \"Using initial parameters provided by strategy\")\n",
    "\n",
    "            self.global_cv = [\n",
    "                np.zeros_like(param, dtype=np.float32) for param in parameters_to_ndarrays(parameters)\n",
    "            ]\n",
    "\n",
    "            return parameters\n",
    "        \n",
    "        log(WARNING, \"No initial parameters provided by strategy, shutting down\")\n",
    "        self.disconnect_all_clients()\n",
    "\n",
    "\n",
    "    def fit_round(\n",
    "            self,\n",
    "            server_round: int,\n",
    "            timeout: Optional[float],\n",
    "            ) -> Optional[Tuple[Optional[Parameters], Dict[str, Scalar], FitResultsAndFailures]]:\n",
    "        \n",
    "        # define client instructions to be passed to \"fit_clients\" function\n",
    "        client_instructions = self.strategy.configure_fit(\n",
    "            server_round=server_round,\n",
    "            parameters=concat_params(self.parameters, self.global_cv),\n",
    "            client_manager=self._client_manager,\n",
    "        )\n",
    "\n",
    "        # if no clients are selected, return None\n",
    "        if not client_instructions:\n",
    "            log(INFO, f\"fit_round {server_round}: no clients selected.\")\n",
    "            return None\n",
    "        \n",
    "        log(\n",
    "            DEBUG,\n",
    "            f\"fit_round {server_round}: selected {len(client_instructions)} clients.\",\n",
    "        )\n",
    "\n",
    "        # Call the \"fit_clients\" function to perform the training on selected clients\n",
    "        results, failures = fit_clients(\n",
    "            client_instructions=client_instructions,\n",
    "            max_workers=self.max_workers,\n",
    "            timeout=timeout,\n",
    "            group_id=server_round,\n",
    "        )\n",
    "\n",
    "        log(DEBUG,\n",
    "            f\"fit_round {server_round}: received {len(results)} results and {len(failures)} failures.\",\n",
    "        )\n",
    "\n",
    "        # Aggregate the results from the clients\n",
    "        aggregated_results = self.strategy.aggregate_fit(\n",
    "            server_round=server_round,\n",
    "            results=results,\n",
    "            failures=failures,\n",
    "        )\n",
    "\n",
    "        # Extract the aggregated parameters and control variates\n",
    "        aggregated_results_combined = []\n",
    "        if aggregated_results[0] is not None:\n",
    "            aggregated_results_combined = parameters_to_ndarrays(aggregated_results[0])\n",
    "\n",
    "        # Split the aggregated results into model parameters and control variates\n",
    "        aggregated_parameters = aggregated_results_combined[:len(aggregated_results_combined) // 2] # model parameters\n",
    "        aggregated_cv = aggregated_results_combined[len(aggregated_results_combined) // 2:]         # control variates\n",
    "\n",
    "        # define the update coefficient for the control variates\n",
    "        cv_coeff = len(results) / len(self._client_manager.all())\n",
    "\n",
    "        # Update the global control variates according to\n",
    "        # global_cv <- global_cv + cv_coeff * aggregated_cv\n",
    "        # where cv_coeff = |S| / N, |S| is the number of clients that participated in the round\n",
    "        # and aggregated_cv = (1 / |S|) * sum_{i in S} (c_i^+ - c_i)\n",
    "        self.global_cv = [\n",
    "            cv + cv_coeff * new_cv for cv, new_cv in zip(self.global_cv, aggregated_cv)\n",
    "        ]\n",
    "\n",
    "\n",
    "        # Update the global model parameters\n",
    "        # new_parameters = current_parameters + aggregated_parameters\n",
    "        # where current_parameters are the parameters of the global model before the round\n",
    "        # and aggregated_parameters = (1 / |S|) * sum_{i in S} (w_i^+ - w)\n",
    "        current_parameters = parameters_to_ndarrays(self.parameters)\n",
    "        new_parameters = [\n",
    "            param + update for param, update in zip(current_parameters, aggregated_parameters)\n",
    "        ]\n",
    "\n",
    "        new_parameters = ndarrays_to_parameters(new_parameters)\n",
    "\n",
    "        return new_parameters, aggregated_results[1], (results, failures)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eebdbc17",
   "metadata": {},
   "source": [
    "We are now ready for the server"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "139ad0e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an instance of the model and get the parameters\n",
    "params = get_parameters(SmallCNN())\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# The `evaluate` function will be called by Flower after every round\n",
    "def evaluate(\n",
    "    server_round: int,\n",
    "    parameters: list[np.ndarray],\n",
    "    config: dict[str, Scalar],\n",
    "    ) -> Optional[Tuple[float, dict[str, Scalar]]]:\n",
    "    \n",
    "    net = SmallCNN().to(DEVICE)\n",
    "    _, _, testloader = load_datasets(0, NUM_PARTITIONS)\n",
    "    set_parameters(net, parameters)  # Update model with the latest parameters\n",
    "    loss, accuracy = test(\n",
    "        net=net, \n",
    "        device=DEVICE, \n",
    "        testloader=testloader, \n",
    "        criterion=criterion,\n",
    "        )\n",
    "    print(f\"Server-side evaluation loss {loss} / accuracy {accuracy}\")\n",
    "    return loss, {\"accuracy\": accuracy}\n",
    "\n",
    "\n",
    "\n",
    "def server_fn(context: Context) -> ServerAppComponents:\n",
    "    # Create FedAvg strategy\n",
    "    strategy = ScaffoldStrategy(\n",
    "        total_num_clients=NUM_PARTITIONS,       # Total number of clients\n",
    "        fraction_fit=1.0,                       # Use all clients for training, C\n",
    "        fraction_evaluate=0.5,                  # Use 50% of clients for evaluation\n",
    "        min_fit_clients=10,                     # Minimum number of clients to train\n",
    "        min_evaluate_clients=5,                 # Minimum number of clients to evaluate\n",
    "        min_available_clients=NUM_PARTITIONS,   # Minimum number of clients available (enforce all clients to be available)\n",
    "        evaluate_fn=evaluate,                   # Pass the evaluation function\n",
    "    )\n",
    "\n",
    "    server = ScaffoldServer(strategy=strategy)\n",
    "\n",
    "    # Configure the server for 3 rounds of training\n",
    "    config = ServerConfig(num_rounds=50)\n",
    "    return ServerAppComponents(server=server, config=config)\n",
    "\n",
    "# Create the ServerApp\n",
    "server = ServerApp(server_fn=server_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85531a44",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92mINFO \u001b[0m:      Starting Flower ServerApp, config: num_rounds=50, no round_timeout\n",
      "\u001b[92mINFO \u001b[0m:      \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92mINFO \u001b[0m:      [INIT]\n",
      "\u001b[92mINFO \u001b[0m:      Using initial parameters provided by strategy\n",
      "\u001b[92mINFO \u001b[0m:      Starting evaluation of initial global parameters\n",
      "\u001b[92mINFO \u001b[0m:      initial parameters (loss, other metrics): 2.3034996201816837, {'accuracy': 0.0996}\n",
      "\u001b[92mINFO \u001b[0m:      \n",
      "\u001b[92mINFO \u001b[0m:      [ROUND 1]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Server-side evaluation loss 2.3034996201816837 / accuracy 0.0996\n"
     ]
    }
   ],
   "source": [
    "NUM_PARTITIONS = 10  # Number of partitions (clients)\n",
    "backend_config = {\n",
    "    #\"ray_init_args\": {\n",
    "    #    \"num_cpus\": 1,\n",
    "    #    \"num_gpus\": 1,\n",
    "    #},\n",
    "    \"client_resources\": {\n",
    "        \"num_cpus\": 2,\n",
    "        \"num_gpus\": 0.2,\n",
    "    }\n",
    "}\n",
    "run_simulation(\n",
    "    server_app=server, client_app=client, num_supernodes=NUM_PARTITIONS, backend_config=backend_config,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20a59d38",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "FLenv2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
