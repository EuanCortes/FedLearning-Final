{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "887c905b",
   "metadata": {},
   "source": [
    "# Federated Learning with Flower, PyTorch, and CIFAR-10\n",
    "\n",
    "This notebook demonstrates how to set up a federated learning pipeline using the [Flower](https://flower.dev/) framework with PyTorch and the CIFAR-10 dataset. Federated learning allows multiple clients to collaboratively train a machine learning model while keeping their data local and private.\n",
    "\n",
    "To promote modularity and reusability, the notebook is structured to cleanly separate the components that are shared between centralized and federated training (e.g., model architecture, training logic) from those specific to the federated setup (e.g., client/server logic, simulation). This design makes it easy to benchmark, test, and iterate across training modes.\n",
    "\n",
    "### Notebook Structure\n",
    "\n",
    "1. **Setup and Imports** – Install dependencies and import core libraries.\n",
    "2. **Dataset Preparation** – Load and partition CIFAR-10 using Flower Datasets and IID partitioning.\n",
    "3. **Training and Evaluation Functions** – Define reusable model training and testing logic for both centralized and federated workflows.\n",
    "4. **ClientApp** – Implement a federated Flower client using the shared training/evaluation logic.\n",
    "5. **ServerApp** – Configure the federated learning strategy and server behavior.\n",
    "6. **Simulation** – Run a federated learning simulation across multiple clients.\n",
    "7. **Evaluation** – Evaluate the final global model performance after training.\n",
    "\n",
    "> Note: This notebook uses simulated clients and centralized coordination for demonstration purposes.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e07a5c91",
   "metadata": {},
   "source": [
    "## 1. Setup and Imports\n",
    "\n",
    "In this section, we install and import the required libraries for our federated learning setup.\n",
    "\n",
    "- **Flower (flwr)**: A framework for building federated learning systems.\n",
    "- **Flower Datasets (flwr_datasets)**: Utilities to download and partition datasets easily.\n",
    "- **PyTorch**: Used for building and training the neural network.\n",
    "- **Predefined CNN model**: Imported from `fedlearn.model`.\n",
    "\n",
    "We will use `FederatedDataset` and `IidPartitioner` from Flower Datasets to partition CIFAR-10 into multiple IID client datasets.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4efec14a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-30 16:17:27.918826: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training on cpu\n",
      "Flower 1.17.0 / PyTorch 2.2.2\n"
     ]
    }
   ],
   "source": [
    "from collections import OrderedDict\n",
    "from typing import Dict, List, Optional, Tuple\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "import flwr\n",
    "from flwr.client import Client, ClientApp, NumPyClient\n",
    "from flwr.server import ServerApp, ServerConfig, ServerAppComponents\n",
    "from flwr.server.strategy import FedAvg, FedAdagrad\n",
    "from flwr.simulation import run_simulation\n",
    "from flwr_datasets import FederatedDataset\n",
    "from flwr.common import ndarrays_to_parameters, NDArrays, Scalar, Context\n",
    "\n",
    "DEVICE = torch.device(\"cpu\")  # Try \"cuda\" to train on GPU\n",
    "print(f\"Training on {DEVICE}\")\n",
    "print(f\"Flower {flwr.__version__} / PyTorch {torch.__version__}\")\n",
    "\n",
    "from fedlearn.model import SmallCNN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a8e6322",
   "metadata": {},
   "source": [
    "## 2. Dataset Preparation\n",
    "\n",
    "In this section, we load and partition the CIFAR-10 dataset using the Flower Datasets library.\n",
    "\n",
    "We perform the following steps:\n",
    "- Download CIFAR-10 using `FederatedDataset`.\n",
    "- Apply standard normalization and transformation.\n",
    "- Use `IidPartitioner` to create IID partitions of the dataset, simulating multiple clients in a federated learning setup.\n",
    "\n",
    "Each partition corresponds to a different client in the federated system.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "26876dc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_PARTITIONS = 10 # Number of partitions for the federated dataset same as the number of clients\n",
    "BATCH_SIZE = 32\n",
    "\n",
    "\n",
    "def load_datasets(partition_id: int, num_partitions: int):\n",
    "    fds = FederatedDataset(dataset=\"cifar10\", partitioners={\"train\": num_partitions})\n",
    "    partition = fds.load_partition(partition_id)\n",
    "    # Divide data on each node: 80% train, 20% test\n",
    "    partition_train_test = partition.train_test_split(test_size=0.2, seed=42)\n",
    "    pytorch_transforms = transforms.Compose(\n",
    "        [transforms.ToTensor(), transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))]\n",
    "    )\n",
    "\n",
    "    def apply_transforms(batch):\n",
    "        # Instead of passing transforms to CIFAR10(..., transform=transform)\n",
    "        # we will use this function to dataset.with_transform(apply_transforms)\n",
    "        # The transforms object is exactly the same\n",
    "        batch[\"img\"] = [pytorch_transforms(img) for img in batch[\"img\"]]\n",
    "        return batch\n",
    "\n",
    "    partition_train_test = partition_train_test.with_transform(apply_transforms)\n",
    "    trainloader = DataLoader(\n",
    "        partition_train_test[\"train\"], batch_size=BATCH_SIZE, shuffle=True\n",
    "    )\n",
    "    valloader = DataLoader(partition_train_test[\"test\"], batch_size=BATCH_SIZE)\n",
    "    testset = fds.load_split(\"test\").with_transform(apply_transforms)\n",
    "    testloader = DataLoader(testset, batch_size=BATCH_SIZE)\n",
    "    return trainloader, valloader, testloader"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59b76a02",
   "metadata": {},
   "source": [
    "### 2.1 Visualizing Label Distribution Across Clients\n",
    "\n",
    "To better understand the data each client sees, we visualize the class label distribution with clients on the x-axis. Each bar now represents the number of samples of a particular class on that client.\n",
    "\n",
    "Since we're using IID partitioning, we expect each class to be fairly evenly distributed across all clients.\n",
    "\n",
    "This visualization helps verify that the `IidPartitioner` produces balanced partitions with representative data for each client.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e769d4e",
   "metadata": {},
   "source": [
    "## 3. Training and Evaluation Functions\n",
    "\n",
    "To support both centralized and federated learning, we define reusable functions for training and evaluating models.\n",
    "\n",
    "These functions can be invoked in two contexts:\n",
    "- Centrally, to train and evaluate a model using all available data (e.g. for benchmarking).\n",
    "- Locally on each federated client, for on-device training and reporting.\n",
    "\n",
    "The training function supports:\n",
    "- Configurable optimizer and hyperparameters\n",
    "- Batch-based model updates\n",
    "\n",
    "The evaluation function returns both loss and accuracy metrics.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "da6aa547",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_parameters(net) -> List[np.ndarray]:\n",
    "    return [val.cpu().numpy() for _, val in net.state_dict().items()]\n",
    "\n",
    "\n",
    "def set_parameters(net, parameters: List[np.ndarray]):\n",
    "    params_dict = zip(net.state_dict().keys(), parameters)\n",
    "    state_dict = OrderedDict({k: torch.Tensor(v) for k, v in params_dict})\n",
    "    net.load_state_dict(state_dict, strict=True)\n",
    "\n",
    "\n",
    "def train(net, trainloader, epochs: int):\n",
    "    \"\"\"Train the network on the training set.\"\"\"\n",
    "    criterion = torch.nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.Adam(net.parameters())\n",
    "    net.train()\n",
    "    for epoch in range(epochs):\n",
    "        correct, total, epoch_loss = 0, 0, 0.0\n",
    "        for batch in trainloader:\n",
    "            images, labels = batch[\"img\"], batch[\"label\"]\n",
    "            images, labels = images.to(DEVICE), labels.to(DEVICE)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = net(images)\n",
    "            loss = criterion(net(images), labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            # Metrics\n",
    "            epoch_loss += loss\n",
    "            total += labels.size(0)\n",
    "            correct += (torch.max(outputs.data, 1)[1] == labels).sum().item()\n",
    "        epoch_loss /= len(trainloader.dataset)\n",
    "        epoch_acc = correct / total\n",
    "        print(f\"Epoch {epoch+1}: train loss {epoch_loss}, accuracy {epoch_acc}\")\n",
    "\n",
    "\n",
    "def test(net, testloader):\n",
    "    \"\"\"Evaluate the network on the entire test set.\"\"\"\n",
    "    criterion = torch.nn.CrossEntropyLoss()\n",
    "    correct, total, loss = 0, 0, 0.0\n",
    "    net.eval()\n",
    "    with torch.no_grad():\n",
    "        for batch in testloader:\n",
    "            images, labels = batch[\"img\"], batch[\"label\"]\n",
    "            images, labels = images.to(DEVICE), labels.to(DEVICE)\n",
    "            outputs = net(images)\n",
    "            loss += criterion(outputs, labels).item()\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "    loss /= len(testloader.dataset)\n",
    "    accuracy = correct / total\n",
    "    return loss, accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c555a43c",
   "metadata": {},
   "source": [
    "## 4. ClientApp\n",
    "\n",
    "Here we define the Flower client, which encapsulates the logic each federated client uses to train and evaluate the model.\n",
    "\n",
    "Each client performs the following:\n",
    "- Receives global model weights from the server.\n",
    "- Trains the model locally on its own partitioned dataset.\n",
    "- Sends the updated weights back to the server.\n",
    "- Evaluates the global model on its local test data.\n",
    "\n",
    "This class makes use of the shared `train` and `test` functions defined earlier to keep the logic consistent and reusable across different training scenarios.\n",
    "\n",
    "Common customization points:\n",
    "- Adjusting the optimizer configuration inside `train()`.\n",
    "- Changing the number of local training epochs or batch size.\n",
    "- Extending evaluation logic with custom metrics.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "85808a4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FlowerClient(NumPyClient):\n",
    "    def __init__(self, partition_id, net, trainloader, valloader):\n",
    "        self.partition_id = partition_id\n",
    "        self.net = net\n",
    "        self.trainloader = trainloader\n",
    "        self.valloader = valloader\n",
    "\n",
    "    def get_parameters(self, config):\n",
    "        print(f\"[Client {self.partition_id}] get_parameters\")\n",
    "        return get_parameters(self.net)\n",
    "\n",
    "    def fit(self, parameters, config):\n",
    "        print(f\"[Client {self.partition_id}] fit, config: {config}\")\n",
    "        set_parameters(self.net, parameters)\n",
    "        train(self.net, self.trainloader, epochs=1)\n",
    "        return get_parameters(self.net), len(self.trainloader), {}\n",
    "\n",
    "    def evaluate(self, parameters, config):\n",
    "        print(f\"[Client {self.partition_id}] evaluate, config: {config}\")\n",
    "        set_parameters(self.net, parameters)\n",
    "        loss, accuracy = test(self.net, self.valloader)\n",
    "        return float(loss), len(self.valloader), {\"accuracy\": float(accuracy)}\n",
    "\n",
    "\n",
    "def client_fn(context: Context) -> Client:\n",
    "    net = SmallCNN().to(DEVICE)\n",
    "\n",
    "    # Read the node_config to fetch data partition associated to this node\n",
    "    partition_id = context.node_config[\"partition-id\"]\n",
    "    num_partitions = context.node_config[\"num-partitions\"]\n",
    "\n",
    "    trainloader, valloader, _ = load_datasets(partition_id, num_partitions)\n",
    "    return FlowerClient(partition_id, net, trainloader, valloader).to_client()\n",
    "\n",
    "\n",
    "# Create the ClientApp\n",
    "client = ClientApp(client_fn=client_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40e3847f",
   "metadata": {},
   "source": [
    "## 5. ServerApp\n",
    "\n",
    "This section defines the server application using Flower's `server_fn` API. The server is responsible for:\n",
    "\n",
    "- Configuring the federated learning strategy (e.g. FedAvg).\n",
    "- Reading runtime settings such as number of rounds and client participation ratio from a configuration context.\n",
    "- Returning the strategy and configuration needed to run the simulation.\n",
    "\n",
    "This approach makes the server logic highly modular and allows easy injection of config values like `num-server-rounds` or `fraction-fit` during runtime.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f3866dc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an instance of the model and get the parameters\n",
    "params = get_parameters(SmallCNN())\n",
    "\n",
    "# The `evaluate` function will be called by Flower after every round\n",
    "def evaluate(\n",
    "    server_round: int,\n",
    "    parameters: NDArrays,\n",
    "    config: Dict[str, Scalar],\n",
    ") -> Optional[Tuple[float, Dict[str, Scalar]]]:\n",
    "    net = SmallCNN().to(DEVICE)\n",
    "    _, _, testloader = load_datasets(0, NUM_PARTITIONS)\n",
    "    set_parameters(net, parameters)  # Update model with the latest parameters\n",
    "    loss, accuracy = test(net, testloader)\n",
    "    print(f\"Server-side evaluation loss {loss} / accuracy {accuracy}\")\n",
    "    return loss, {\"accuracy\": accuracy}\n",
    "\n",
    "def server_fn(context: Context) -> ServerAppComponents:\n",
    "    # Create FedAvg strategy\n",
    "    strategy = FedAvg(\n",
    "        fraction_fit=1.0, # Use all clients for training, C\n",
    "        fraction_evaluate=0.5, # Use 50% of clients for evaluation\n",
    "        min_fit_clients=10,  # Minimum number of clients to train\n",
    "        min_evaluate_clients=5,\n",
    "        min_available_clients=NUM_PARTITIONS,\n",
    "        initial_parameters=ndarrays_to_parameters(\n",
    "            params\n",
    "        ),  # Pass initial model parameters\n",
    "        evaluate_fn=evaluate,  # Pass the evaluation function\n",
    "    )\n",
    "\n",
    "    # Configure the server for 3 rounds of training\n",
    "    config = ServerConfig(num_rounds=10)\n",
    "    return ServerAppComponents(strategy=strategy, config=config)\n",
    "\n",
    "# Create the ServerApp\n",
    "server = ServerApp(server_fn=server_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5488e440",
   "metadata": {},
   "source": [
    "## 6. Simulation\n",
    "\n",
    "With all components in place, we now simulate the federated learning process using Flower’s `start_simulation` function.\n",
    "\n",
    "This includes:\n",
    "- Initializing each client with its own data partition.\n",
    "- Launching multiple client instances in parallel using `ClientApp`.\n",
    "- Running the simulation across a specified number of federated rounds.\n",
    "\n",
    "We control how many clients participate per round and how many rounds of training we perform.\n",
    "\n",
    "Common configuration options:\n",
    "- Number of clients in the federation\n",
    "- Number of training rounds\n",
    "- Client resources (e.g., number of CPUs or GPUs per client)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78414882",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92mINFO \u001b[0m:      Starting Flower ServerApp, config: num_rounds=10, no round_timeout\n",
      "\u001b[92mINFO \u001b[0m:      \n",
      "\u001b[92mINFO \u001b[0m:      [INIT]\n",
      "\u001b[92mINFO \u001b[0m:      Using initial global parameters provided by strategy\n",
      "\u001b[92mINFO \u001b[0m:      Starting evaluation of initial global parameters\n",
      "\u001b[92mINFO \u001b[0m:      initial parameters (loss, other metrics): 0.07212714471817017, {'accuracy': 0.1}\n",
      "\u001b[92mINFO \u001b[0m:      \n",
      "\u001b[92mINFO \u001b[0m:      [ROUND 1]\n",
      "\u001b[92mINFO \u001b[0m:      configure_fit: strategy sampled 10 clients (out of 10)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Server-side evaluation loss 0.07212714471817017 / accuracy 0.1\n",
      "\u001b[36m(ClientAppActor pid=13967)\u001b[0m [Client 2] fit, config: {}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "NUM_PARTITIONS = 10  # Number of partitions (clients)\n",
    "run_simulation(\n",
    "    server_app=server, client_app=client, num_supernodes=NUM_PARTITIONS\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcfc2dac",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
